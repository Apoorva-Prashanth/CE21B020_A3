{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-05-20T18:09:52.888979Z",
     "iopub.status.busy": "2025-05-20T18:09:52.888476Z",
     "iopub.status.idle": "2025-05-20T18:09:53.173254Z",
     "shell.execute_reply": "2025-05-20T18:09:53.172511Z",
     "shell.execute_reply.started": "2025-05-20T18:09:52.888960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mapoorvaprashanth\u001b[0m (\u001b[33mapoorvaprashanth-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"give wandb api key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanilla model for initial sweep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Run the below cell to sweep, sweep configuration can be chnaged based on observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-18T14:07:09.091016Z",
     "iopub.status.busy": "2025-05-18T14:07:09.090411Z",
     "iopub.status.idle": "2025-05-18T15:40:36.249260Z",
     "shell.execute_reply": "2025-05-18T15:40:36.248292Z",
     "shell.execute_reply.started": "2025-05-18T14:07:09.090991Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 200n7n5k\n",
      "Sweep URL: https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 34h930t9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_140720-34h930t9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/34h930t9' target=\"_blank\">colorful-sweep-1</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/34h930t9' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/34h930t9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.6122, Seq Acc = 0.0893, Token Acc = 0.4890\n",
      "Epoch 2: Train Loss = 0.9561, Seq Acc = 0.2204, Token Acc = 0.6336\n",
      "Epoch 3: Train Loss = 0.7857, Seq Acc = 0.2569, Token Acc = 0.6603\n",
      "Epoch 4: Train Loss = 0.7063, Seq Acc = 0.2725, Token Acc = 0.6683\n",
      "Epoch 5: Train Loss = 0.6542, Seq Acc = 0.2882, Token Acc = 0.6960\n",
      "Epoch 6: Train Loss = 0.6128, Seq Acc = 0.3056, Token Acc = 0.7082\n",
      "Epoch 7: Train Loss = 0.5903, Seq Acc = 0.3175, Token Acc = 0.7176\n",
      "Epoch 8: Train Loss = 0.5729, Seq Acc = 0.3065, Token Acc = 0.7091\n",
      "Epoch 9: Train Loss = 0.5529, Seq Acc = 0.3269, Token Acc = 0.7223\n",
      "Epoch 10: Train Loss = 0.5390, Seq Acc = 0.3283, Token Acc = 0.7225\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇▇█▇██</td></tr><tr><td>val_token_accuracy</td><td>▁▅▆▆▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.53905</td></tr><tr><td>val_accuracy</td><td>0.32831</td></tr><tr><td>val_token_accuracy</td><td>0.72252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed256_hid64_enc2_dec1_LSTM_drop0.3_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/34h930t9' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/34h930t9</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_140720-34h930t9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vd5e0t10 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_143710-vd5e0t10</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/vd5e0t10' target=\"_blank\">wild-sweep-2</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/vd5e0t10' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/vd5e0t10</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.5899, Seq Acc = 0.1377, Token Acc = 0.5648\n",
      "Epoch 2: Train Loss = 0.9084, Seq Acc = 0.2351, Token Acc = 0.6516\n",
      "Epoch 3: Train Loss = 0.7481, Seq Acc = 0.2650, Token Acc = 0.6665\n",
      "Epoch 4: Train Loss = 0.6741, Seq Acc = 0.2854, Token Acc = 0.6790\n",
      "Epoch 5: Train Loss = 0.6257, Seq Acc = 0.2913, Token Acc = 0.6932\n",
      "Epoch 6: Train Loss = 0.5972, Seq Acc = 0.3210, Token Acc = 0.7129\n",
      "Epoch 7: Train Loss = 0.5711, Seq Acc = 0.3171, Token Acc = 0.7089\n",
      "Epoch 8: Train Loss = 0.5546, Seq Acc = 0.3221, Token Acc = 0.7171\n",
      "Epoch 9: Train Loss = 0.5385, Seq Acc = 0.3264, Token Acc = 0.7200\n",
      "Epoch 10: Train Loss = 0.5259, Seq Acc = 0.3251, Token Acc = 0.7188\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▆▇█████</td></tr><tr><td>val_token_accuracy</td><td>▁▅▆▆▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.52593</td></tr><tr><td>val_accuracy</td><td>0.32512</td></tr><tr><td>val_token_accuracy</td><td>0.71876</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid64_enc2_dec1_LSTM_drop0.2_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/vd5e0t10' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/vd5e0t10</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_143710-vd5e0t10/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0whhxb57 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_150642-0whhxb57</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/0whhxb57' target=\"_blank\">usual-sweep-3</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/0whhxb57' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/0whhxb57</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.6110, Seq Acc = 0.1048, Token Acc = 0.5255\n",
      "Epoch 2: Train Loss = 0.9154, Seq Acc = 0.2260, Token Acc = 0.6362\n",
      "Epoch 3: Train Loss = 0.7561, Seq Acc = 0.2638, Token Acc = 0.6708\n",
      "Epoch 4: Train Loss = 0.6743, Seq Acc = 0.2902, Token Acc = 0.7009\n",
      "Epoch 5: Train Loss = 0.6295, Seq Acc = 0.2980, Token Acc = 0.6974\n",
      "Epoch 6: Train Loss = 0.5924, Seq Acc = 0.3092, Token Acc = 0.7098\n",
      "Epoch 7: Train Loss = 0.5702, Seq Acc = 0.3170, Token Acc = 0.7115\n",
      "Epoch 8: Train Loss = 0.5503, Seq Acc = 0.3221, Token Acc = 0.7184\n",
      "Epoch 9: Train Loss = 0.5374, Seq Acc = 0.3281, Token Acc = 0.7222\n",
      "Epoch 10: Train Loss = 0.5221, Seq Acc = 0.3327, Token Acc = 0.7212\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇████</td></tr><tr><td>val_token_accuracy</td><td>▁▅▆▇▇█████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.52205</td></tr><tr><td>val_accuracy</td><td>0.33274</td></tr><tr><td>val_token_accuracy</td><td>0.72118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid64_enc2_dec1_LSTM_drop0.2_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/0whhxb57' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/0whhxb57</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250518_150642-0whhxb57/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kbknrf4b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250518_153612-kbknrf4b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/kbknrf4b' target=\"_blank\">fancy-sweep-4</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/200n7n5k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/kbknrf4b' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/kbknrf4b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.2823, Seq Acc = 0.2523, Token Acc = 0.6787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Char-level vocabulary\n",
    "class CharVocab:\n",
    "    \"\"\"\n",
    "    A character-level vocabulary class for encoding and decoding sequences of characters.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    char2idx : dict\n",
    "        Mapping from characters to integer indices. Includes special tokens:\n",
    "        '<pad>' (0), '<sos>' (1), '<eos>' (2), and '<unk>' (3).\n",
    "        \n",
    "    idx2char : dict\n",
    "        Reverse mapping from indices to characters.\n",
    "\n",
    "    pad_idx : int\n",
    "        Index of the padding token ('<pad>').\n",
    "\n",
    "    sos_idx : int\n",
    "        Index of the start-of-sequence token ('<sos>').\n",
    "\n",
    "    eos_idx : int\n",
    "        Index of the end-of-sequence token ('<eos>').\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    encode(word: str) -> List[int]\n",
    "        Converts a string into a list of indices, including <sos> at the start and <eos> at the end.\n",
    "        Unknown characters are mapped to the <unk> index.\n",
    "\n",
    "    decode(ids: List[int]) -> str\n",
    "        Converts a list of indices back into a string, ignoring <sos> and <pad>, and stopping at <eos>.\n",
    "\n",
    "    __len__() -> int\n",
    "        Returns the size of the vocabulary (i.e., number of unique tokens including special tokens).\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        chars = sorted(set(\"\".join(words)))\n",
    "        self.char2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        for c in chars:\n",
    "            self.char2idx[c] = len(self.char2idx)\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.pad_idx = self.char2idx['<pad>']\n",
    "        self.sos_idx = self.char2idx['<sos>']\n",
    "        self.eos_idx = self.char2idx['<eos>']\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.sos_idx] + [self.char2idx.get(c, self.char2idx['<unk>']) for c in word] + [self.eos_idx]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx == self.eos_idx:\n",
    "                break\n",
    "            if idx not in (self.sos_idx, self.pad_idx):\n",
    "                chars.append(self.idx2char.get(idx, ''))\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "# Load data\n",
    "def read_file(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if len(line.split('\\t')) >= 2]\n",
    "\n",
    "train_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.train.tsv')\n",
    "dev_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.dev.tsv')\n",
    "test_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.test.tsv')\n",
    "\n",
    "src_vocab = CharVocab([src for src, _ in train_pairs])\n",
    "tgt_vocab = CharVocab([tgt for _, tgt in train_pairs])\n",
    "\n",
    "# Dataset\n",
    "class TransliterationDataset(Dataset):\n",
    "    \"\"\"\n",
    "This code defines a PyTorch dataset and dataloaders for a character-level transliteration task. It converts input-output string pairs into sequences of token indices using source and target vocabularies, and pads them for batch processing. The TransliterationDataset encodes each word pair, while the collate_fn ensures proper padding during batching. Dataloaders are created for training and validation with appropriate batch sizes.\n",
    "\"\"\"\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.data = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        return torch.tensor(self.src_vocab.encode(src)), torch.tensor(self.tgt_vocab.encode(tgt))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "train_loader = DataLoader(TransliterationDataset(train_pairs, src_vocab, tgt_vocab),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(TransliterationDataset(dev_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"\n",
    "    A sequence-to-sequence model for character-level transliteration using RNNs.\"\"\"\n",
    "    def __init__(self, config, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_enc_layers = config.enc_layers\n",
    "        self.num_dec_layers = config.dec_layers\n",
    "        self.cell_type = config.cell\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.max_len = 30\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, self.embedding_dim)\n",
    "\n",
    "        RNN = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]\n",
    "        self.encoder = RNN(self.embedding_dim, self.hidden_size, num_layers=self.num_enc_layers,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.decoder = RNN(self.embedding_dim, self.hidden_size * 2, num_layers=self.num_dec_layers,\n",
    "                           batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, output_vocab_size)\n",
    "\n",
    "        self.sos_idx = tgt_vocab.sos_idx\n",
    "        self.eos_idx = tgt_vocab.eos_idx\n",
    "        self.pad_idx = tgt_vocab.pad_idx\n",
    "\n",
    "    def encode(self, src): \n",
    "        '''Encodes the input sequence using the encoder RNN.\n",
    "        Args: '''\n",
    "        embedded = self.dropout(self.encoder_embedding(src))\n",
    "        outputs, h_n = self.encoder(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h, c = h_n\n",
    "            h_cat = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)\n",
    "            c_cat = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, (h_cat, c_cat)\n",
    "        else:\n",
    "            h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, h_cat\n",
    "\n",
    "    def decode_step(self, input_token, hidden):\n",
    "        embedded = self.dropout(self.decoder_embedding(input_token))\n",
    "        output, hidden = self.decoder(embedded, hidden)\n",
    "        logits = self.fc(output.squeeze(1))\n",
    "        return logits, hidden\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        _, hidden = self.encode(src)\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decode_step(input_token, hidden)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation\n",
    "def beam_decode(model, src, beam_size):\n",
    "    \"\"\"\n",
    "    Beam search decoding for the Seq2Seq model.\n",
    "    Args:\n",
    "        model: The trained Seq2Seq model.\n",
    "        src: The source sequence tensor.\n",
    "        beam_size: The number of beams to keep during decoding.\n",
    "        Returns:\n",
    "        A list of predicted sequences.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encode(src)\n",
    "        batch_size = src.size(0)\n",
    "        final_outputs = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            h_b = (hidden[0][:, b:b+1, :].contiguous(), hidden[1][:, b:b+1, :].contiguous()) if model.cell_type == 'LSTM' else hidden[:, b:b+1, :].contiguous()\n",
    "            beams = [([model.sos_idx], 0.0, h_b)]\n",
    "            for _ in range(model.max_len):\n",
    "                new_beams = []\n",
    "                for seq, score, h in beams:\n",
    "                    if seq[-1] == model.eos_idx:\n",
    "                        new_beams.append((seq, score, h))\n",
    "                        continue\n",
    "                    input_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                    out, h_new = model.decode_step(input_token, h)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    topk_probs, topk_idxs = torch.topk(log_probs, beam_size, dim=1)\n",
    "                    for i in range(beam_size):\n",
    "                        next_seq = seq + [topk_idxs[0][i].item()]\n",
    "                        new_score = score + topk_probs[0][i].item()\n",
    "                        new_beams.append((next_seq, new_score, h_new))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            final_outputs.append(beams[0][0])\n",
    "        return final_outputs\n",
    "\n",
    "def evaluate_beam(model, dataloader, beam_size):\n",
    "    \"\"\"\n",
    "    Evaluate the model using beam search decoding.\n",
    "    Args:\n",
    "        model: The trained Seq2Seq model.\n",
    "        dataloader: DataLoader for the evaluation dataset.\n",
    "        beam_size: The number of beams to keep during decoding.\n",
    "        Returns:\n",
    "        Tuple of sequence-level and token-level accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_seq, correct_seq = 0, 0\n",
    "    total_tokens, correct_tokens = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            preds = beam_decode(model, src, beam_size)\n",
    "            for pred, true in zip(preds, tgt):\n",
    "                pred_trimmed = [tok for tok in pred[1:] if tok != model.pad_idx and tok != model.eos_idx]\n",
    "                true_trimmed = [tok.item() for tok in true[1:] if tok.item() != model.pad_idx and tok.item() != model.eos_idx]\n",
    "\n",
    "                # Sequence-level accuracy\n",
    "                if pred_trimmed == true_trimmed:\n",
    "                    correct_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "                # Token-level accuracy\n",
    "                for p, t in zip(pred_trimmed, true_trimmed):\n",
    "                    if p == t:\n",
    "                        correct_tokens += 1\n",
    "                total_tokens += len(true_trimmed)\n",
    "\n",
    "    seq_accuracy = correct_seq / total_seq if total_seq > 0 else 0.0\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    return seq_accuracy, token_accuracy\n",
    "\n",
    "\n",
    "# W&B Sweep Training\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config  # MUST be done before using config\n",
    "\n",
    "        run_name = f\"embed{config.embed_size}_hid{config.hidden_size}_enc{config.enc_layers}_dec{config.dec_layers}_{config.cell}_drop{config.dropout}_beam{config.beam_size}\"\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        model = Seq2Seq(config, len(src_vocab), len(tgt_vocab)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion)\n",
    "            acc, token_acc = evaluate_beam(model, dev_loader, beam_size=config.beam_size)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_accuracy': acc,\n",
    "                'val_token_accuracy': token_acc\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Seq Acc = {acc:.4f}, Token Acc = {token_acc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [128, 256]},\n",
    "        'hidden_size': {'values': [64, 128]},\n",
    "        'enc_layers': {'values': [2]},\n",
    "        'dec_layers': {'values': [1]},\n",
    "        'dropout': {'values': [0.2, 0.3]},\n",
    "        'cell': {'values': ['LSTM']},\n",
    "        'beam_size': {'values': [5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"A3_ce21b020\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Added attention (Q5)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-19T08:35:50.279Z",
     "iopub.execute_input": "2025-05-19T04:37:20.181416Z",
     "iopub.status.busy": "2025-05-19T04:37:20.180840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 97snhvbo\n",
      "Sweep URL: https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: fzpcg94g with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_043732-fzpcg94g</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/fzpcg94g' target=\"_blank\">resilient-sweep-1</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/fzpcg94g' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/fzpcg94g</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8944, Seq Acc = 0.3636, Token Acc = 0.7392\n",
      "Epoch 2: Train Loss = 0.5061, Seq Acc = 0.3755, Token Acc = 0.7446\n",
      "Epoch 3: Train Loss = 0.4734, Seq Acc = 0.3627, Token Acc = 0.7437\n",
      "Epoch 4: Train Loss = 0.4442, Seq Acc = 0.3785, Token Acc = 0.7470\n",
      "Epoch 5: Train Loss = 0.4329, Seq Acc = 0.3673, Token Acc = 0.7468\n",
      "Epoch 6: Train Loss = 0.4186, Seq Acc = 0.3744, Token Acc = 0.7467\n",
      "Epoch 7: Train Loss = 0.4152, Seq Acc = 0.3810, Token Acc = 0.7545\n",
      "Epoch 8: Train Loss = 0.4038, Seq Acc = 0.3744, Token Acc = 0.7472\n",
      "Epoch 9: Train Loss = 0.3994, Seq Acc = 0.3691, Token Acc = 0.7466\n",
      "Epoch 10: Train Loss = 0.3927, Seq Acc = 0.3721, Token Acc = 0.7412\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▆▁▇▃▅█▅▃▅</td></tr><tr><td>val_token_accuracy</td><td>▁▃▃▅▄▄█▅▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.39267</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.3721</td></tr><tr><td>val_token_accuracy</td><td>0.74119</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid128_enc2_dec1_GRU_drop0.25_beam3</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/fzpcg94g' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/fzpcg94g</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_043732-fzpcg94g/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nt29evpt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_050922-nt29evpt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/nt29evpt' target=\"_blank\">graceful-sweep-2</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/nt29evpt' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/nt29evpt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.0539, Seq Acc = 0.3593, Token Acc = 0.7437\n",
      "Epoch 2: Train Loss = 0.5119, Seq Acc = 0.3742, Token Acc = 0.7482\n",
      "Epoch 3: Train Loss = 0.4659, Seq Acc = 0.3712, Token Acc = 0.7464\n",
      "Epoch 4: Train Loss = 0.4379, Seq Acc = 0.3758, Token Acc = 0.7487\n",
      "Epoch 5: Train Loss = 0.4250, Seq Acc = 0.3769, Token Acc = 0.7497\n",
      "Epoch 6: Train Loss = 0.4132, Seq Acc = 0.3691, Token Acc = 0.7459\n",
      "Epoch 7: Train Loss = 0.4066, Seq Acc = 0.3739, Token Acc = 0.7551\n",
      "Epoch 8: Train Loss = 0.3995, Seq Acc = 0.3686, Token Acc = 0.7498\n",
      "Epoch 9: Train Loss = 0.3897, Seq Acc = 0.3698, Token Acc = 0.7464\n",
      "Epoch 10: Train Loss = 0.3793, Seq Acc = 0.3760, Token Acc = 0.7472\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▇▆██▅▇▅▅█</td></tr><tr><td>val_token_accuracy</td><td>▁▄▃▄▅▂█▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.3793</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.376</td></tr><tr><td>val_token_accuracy</td><td>0.74725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed256_hid128_enc1_dec1_LSTM_drop0.25_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/nt29evpt' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/nt29evpt</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_050922-nt29evpt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: f8zw99p4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_055948-f8zw99p4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/f8zw99p4' target=\"_blank\">copper-sweep-3</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/f8zw99p4' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/f8zw99p4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8683, Seq Acc = 0.3762, Token Acc = 0.7537\n",
      "Epoch 2: Train Loss = 0.5146, Seq Acc = 0.3709, Token Acc = 0.7410\n",
      "Epoch 3: Train Loss = 0.4759, Seq Acc = 0.3700, Token Acc = 0.7439\n",
      "Epoch 4: Train Loss = 0.4467, Seq Acc = 0.3815, Token Acc = 0.7559\n",
      "Epoch 5: Train Loss = 0.4321, Seq Acc = 0.3684, Token Acc = 0.7420\n",
      "Epoch 6: Train Loss = 0.4294, Seq Acc = 0.3794, Token Acc = 0.7541\n",
      "Epoch 7: Train Loss = 0.4163, Seq Acc = 0.3717, Token Acc = 0.7470\n",
      "Epoch 8: Train Loss = 0.4056, Seq Acc = 0.3694, Token Acc = 0.7487\n",
      "Epoch 9: Train Loss = 0.4038, Seq Acc = 0.3748, Token Acc = 0.7559\n",
      "Epoch 10: Train Loss = 0.3979, Seq Acc = 0.3664, Token Acc = 0.7450\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▆▃▃█▂▇▃▂▅▁</td></tr><tr><td>val_token_accuracy</td><td>▇▁▂█▁▇▄▅█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.39791</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.36642</td></tr><tr><td>val_token_accuracy</td><td>0.745</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid128_enc2_dec1_GRU_drop0.3_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/f8zw99p4' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/f8zw99p4</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_055948-f8zw99p4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c9iqdtvk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_064958-c9iqdtvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/c9iqdtvk' target=\"_blank\">visionary-sweep-4</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/c9iqdtvk' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/c9iqdtvk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.9719, Seq Acc = 0.3625, Token Acc = 0.7362\n",
      "Epoch 2: Train Loss = 0.5056, Seq Acc = 0.3725, Token Acc = 0.7474\n",
      "Epoch 3: Train Loss = 0.4535, Seq Acc = 0.3765, Token Acc = 0.7470\n",
      "Epoch 4: Train Loss = 0.4334, Seq Acc = 0.3739, Token Acc = 0.7462\n",
      "Epoch 5: Train Loss = 0.4218, Seq Acc = 0.3804, Token Acc = 0.7485\n",
      "Epoch 6: Train Loss = 0.4048, Seq Acc = 0.3909, Token Acc = 0.7555\n",
      "Epoch 7: Train Loss = 0.3883, Seq Acc = 0.3815, Token Acc = 0.7553\n",
      "Epoch 8: Train Loss = 0.3819, Seq Acc = 0.3845, Token Acc = 0.7516\n",
      "Epoch 9: Train Loss = 0.3772, Seq Acc = 0.3774, Token Acc = 0.7527\n",
      "Epoch 10: Train Loss = 0.3743, Seq Acc = 0.3764, Token Acc = 0.7502\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▃▄▄▅█▆▆▅▄</td></tr><tr><td>val_token_accuracy</td><td>▁▅▅▅▅██▇▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.37429</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.37635</td></tr><tr><td>val_token_accuracy</td><td>0.75024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid128_enc2_dec1_LSTM_drop0.25_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/c9iqdtvk' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/c9iqdtvk</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_064958-c9iqdtvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3r5lxpg7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_074105-3r5lxpg7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/3r5lxpg7' target=\"_blank\">ruby-sweep-5</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/3r5lxpg7' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/3r5lxpg7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8944, Seq Acc = 0.3409, Token Acc = 0.7214\n",
      "Epoch 2: Train Loss = 0.5244, Seq Acc = 0.3593, Token Acc = 0.7334\n",
      "Epoch 3: Train Loss = 0.4739, Seq Acc = 0.3643, Token Acc = 0.7419\n",
      "Epoch 4: Train Loss = 0.4524, Seq Acc = 0.3863, Token Acc = 0.7545\n",
      "Epoch 5: Train Loss = 0.4396, Seq Acc = 0.3655, Token Acc = 0.7433\n",
      "Epoch 6: Train Loss = 0.4236, Seq Acc = 0.3616, Token Acc = 0.7492\n",
      "Epoch 7: Train Loss = 0.4153, Seq Acc = 0.3792, Token Acc = 0.7529\n",
      "Epoch 8: Train Loss = 0.4065, Seq Acc = 0.3795, Token Acc = 0.7533\n",
      "Epoch 9: Train Loss = 0.4054, Seq Acc = 0.3760, Token Acc = 0.7521\n",
      "Epoch 10: Train Loss = 0.3966, Seq Acc = 0.3693, Token Acc = 0.7501\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▄▅█▅▄▇▇▆▅</td></tr><tr><td>val_token_accuracy</td><td>▁▄▅█▆▇██▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>train_loss</td><td>0.39665</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.36926</td></tr><tr><td>val_token_accuracy</td><td>0.75011</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed128_hid128_enc2_dec1_GRU_drop0.35_beam3</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/3r5lxpg7' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/3r5lxpg7</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250519_074105-3r5lxpg7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mlk5ph1v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: GRU\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.25\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tused_attention: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250519_081256-mlk5ph1v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/mlk5ph1v' target=\"_blank\">laced-sweep-6</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/97snhvbo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/mlk5ph1v' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/mlk5ph1v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.8739, Seq Acc = 0.3625, Token Acc = 0.7429\n",
      "Epoch 2: Train Loss = 0.5159, Seq Acc = 0.3645, Token Acc = 0.7406\n",
      "Epoch 3: Train Loss = 0.4794, Seq Acc = 0.3707, Token Acc = 0.7508\n",
      "Epoch 4: Train Loss = 0.4531, Seq Acc = 0.3737, Token Acc = 0.7519\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Char-level vocabulary\n",
    "class CharVocab:\n",
    "    \"\"\"\n",
    "    A character-level vocabulary class for encoding and decoding sequences of characters.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    char2idx : dict\n",
    "        Mapping from characters to integer indices. Includes special tokens:\n",
    "        '<pad>' (0), '<sos>' (1), '<eos>' (2), and '<unk>' (3).\n",
    "        \n",
    "    idx2char : dict\n",
    "        Reverse mapping from indices to characters.\n",
    "\n",
    "    pad_idx : int\n",
    "        Index of the padding token ('<pad>').\n",
    "\n",
    "    sos_idx : int\n",
    "        Index of the start-of-sequence token ('<sos>').\n",
    "\n",
    "    eos_idx : int\n",
    "        Index of the end-of-sequence token ('<eos>').\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    encode(word: str) -> List[int]\n",
    "        Converts a string into a list of indices, including <sos> at the start and <eos> at the end.\n",
    "        Unknown characters are mapped to the <unk> index.\n",
    "\n",
    "    decode(ids: List[int]) -> str\n",
    "        Converts a list of indices back into a string, ignoring <sos> and <pad>, and stopping at <eos>.\n",
    "\n",
    "    __len__() -> int\n",
    "        Returns the size of the vocabulary (i.e., number of unique tokens including special tokens).\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        chars = sorted(set(\"\".join(words)))\n",
    "        self.char2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        for c in chars:\n",
    "            self.char2idx[c] = len(self.char2idx)\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.pad_idx = self.char2idx['<pad>']\n",
    "        self.sos_idx = self.char2idx['<sos>']\n",
    "        self.eos_idx = self.char2idx['<eos>']\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.sos_idx] + [self.char2idx.get(c, self.char2idx['<unk>']) for c in word] + [self.eos_idx]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx == self.eos_idx:\n",
    "                break\n",
    "            if idx not in (self.sos_idx, self.pad_idx):\n",
    "                chars.append(self.idx2char.get(idx, ''))\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if len(line.split('\\t')) >= 2]\n",
    "\n",
    "train_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.train.tsv')\n",
    "dev_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.dev.tsv')\n",
    "test_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.test.tsv')\n",
    "\n",
    "src_vocab = CharVocab([src for src, _ in train_pairs])\n",
    "tgt_vocab = CharVocab([tgt for _, tgt in train_pairs])\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    \"\"\"\n",
    "This code defines a PyTorch dataset and dataloaders for a character-level transliteration task. It converts input-output string pairs into sequences of token indices using source and target vocabularies, and pads them for batch processing. The TransliterationDataset encodes each word pair, while the collate_fn ensures proper padding during batching. Dataloaders are created for training and validation with appropriate batch sizes.\n",
    "\"\"\"\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.data = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.data[idx]\n",
    "        return torch.tensor(self.src_vocab.encode(src)), torch.tensor(self.tgt_vocab.encode(tgt))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "train_loader = DataLoader(TransliterationDataset(train_pairs, src_vocab, tgt_vocab),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(TransliterationDataset(dev_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple attention mechanism for the Seq2Seq model.\n",
    "    Args:\n",
    "        enc_hidden_dim (int): The hidden dimension of the encoder.\n",
    "        dec_hidden_dim (int): The hidden dimension of the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attn_weights = torch.bmm(v, energy).squeeze(1)\n",
    "        if mask is not None:\n",
    "            attn_weights = attn_weights.masked_fill(mask == 0, -1e10)\n",
    "        return F.softmax(attn_weights, dim=1)\n",
    "\n",
    "# Seq2Seq with Attention\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_enc_layers = config.enc_layers\n",
    "        self.num_dec_layers = config.dec_layers\n",
    "        self.cell_type = config.cell\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.max_len = 30\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, self.embedding_dim)\n",
    "\n",
    "        RNN = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]\n",
    "        self.encoder = RNN(self.embedding_dim, self.hidden_size, num_layers=self.num_enc_layers,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.decoder = RNN(self.embedding_dim + self.hidden_size * 2, self.hidden_size * 2,\n",
    "                           num_layers=self.num_dec_layers, batch_first=True)\n",
    "\n",
    "        self.attention = Attention(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        self.fc = nn.Linear(self.hidden_size * 4, output_vocab_size)\n",
    "\n",
    "        self.sos_idx = tgt_vocab.sos_idx\n",
    "        self.eos_idx = tgt_vocab.eos_idx\n",
    "        self.pad_idx = tgt_vocab.pad_idx\n",
    "\n",
    "    def encode(self, src):\n",
    "        embedded = self.dropout(self.encoder_embedding(src))\n",
    "        outputs, h_n = self.encoder(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h, c = h_n\n",
    "            h_cat = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)\n",
    "            c_cat = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, (h_cat, c_cat)\n",
    "        else:\n",
    "            h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, h_cat\n",
    "\n",
    "    def decode_step(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.decoder_embedding(input_token))\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h_t = hidden[0][-1]\n",
    "        else:\n",
    "            h_t = hidden[-1]\n",
    "        attn_weights = self.attention(h_t, encoder_outputs)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.decoder(rnn_input, hidden)\n",
    "        logits = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return logits, hidden\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        encoder_outputs, hidden = self.encode(src)\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decode_step(input_token, hidden, encoder_outputs)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Beam Decoding\n",
    "def beam_decode(model, src, beam_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encode(src)\n",
    "        batch_size = src.size(0)\n",
    "        final_outputs = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            h_b = (hidden[0][:, b:b+1, :].contiguous(), hidden[1][:, b:b+1, :].contiguous()) if model.cell_type == 'LSTM' else hidden[:, b:b+1, :].contiguous()\n",
    "            enc_out_b = encoder_outputs[b:b+1]\n",
    "            beams = [([model.sos_idx], 0.0, h_b)]\n",
    "            for _ in range(model.max_len):\n",
    "                new_beams = []\n",
    "                for seq, score, h in beams:\n",
    "                    if seq[-1] == model.eos_idx:\n",
    "                        new_beams.append((seq, score, h))\n",
    "                        continue\n",
    "                    input_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                    out, h_new = model.decode_step(input_token, h, enc_out_b)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    topk_probs, topk_idxs = torch.topk(log_probs, beam_size, dim=1)\n",
    "                    for i in range(beam_size):\n",
    "                        next_seq = seq + [topk_idxs[0][i].item()]\n",
    "                        new_score = score + topk_probs[0][i].item()\n",
    "                        new_beams.append((next_seq, new_score, h_new))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            final_outputs.append(beams[0][0])\n",
    "        return final_outputs\n",
    "\n",
    "def evaluate_beam(model, dataloader, beam_size):\n",
    "    model.eval()\n",
    "    total_seq, correct_seq = 0, 0\n",
    "    total_tokens, correct_tokens = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            preds = beam_decode(model, src, beam_size)\n",
    "            for pred, true in zip(preds, tgt):\n",
    "                pred_trimmed = [tok for tok in pred[1:] if tok != model.pad_idx and tok != model.eos_idx]\n",
    "                true_trimmed = [tok.item() for tok in true[1:] if tok.item() != model.pad_idx and tok.item() != model.eos_idx]\n",
    "\n",
    "                if pred_trimmed == true_trimmed:\n",
    "                    correct_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "                for p, t in zip(pred_trimmed, true_trimmed):\n",
    "                    if p == t:\n",
    "                        correct_tokens += 1\n",
    "                total_tokens += len(true_trimmed)\n",
    "\n",
    "    return correct_seq / total_seq, correct_tokens / total_tokens\n",
    "\n",
    "# W&B Sweep Training\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        run_name = f\"embed{config.embed_size}_hid{config.hidden_size}_enc{config.enc_layers}_dec{config.dec_layers}_{config.cell}_drop{config.dropout}_beam{config.beam_size}\"\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        model = Seq2Seq(config, len(src_vocab), len(tgt_vocab)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "\n",
    "        for epoch in range(10):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion)\n",
    "            acc, token_acc = evaluate_beam(model, dev_loader, beam_size=config.beam_size)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_accuracy': acc,\n",
    "                'val_token_accuracy': token_acc,\n",
    "                'used_attention':config.used_attention\n",
    "            })\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Seq Acc = {acc:.4f}, Token Acc = {token_acc:.4f}\")\n",
    "\n",
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [128, 256]},\n",
    "        'hidden_size': {'values': [128]},\n",
    "        'enc_layers': {'values': [1, 2]},\n",
    "        'dec_layers': {'values': [1]},\n",
    "        'dropout': {'values': [0.25, 0.3, 0.35]},\n",
    "        'cell': {'values': ['GRU','LSTM']},\n",
    "        'beam_size': {'values': [3, 5]},\n",
    "        'used_attention':{'values':[True]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"A3_ce21b020\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**best model without attention**  Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T07:13:20.416348Z",
     "iopub.status.busy": "2025-05-20T07:13:20.415887Z",
     "iopub.status.idle": "2025-05-20T08:59:27.677030Z",
     "shell.execute_reply": "2025-05-20T08:59:27.676456Z",
     "shell.execute_reply.started": "2025-05-20T07:13:20.416325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: hcp01g1t\n",
      "Sweep URL: https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/hcp01g1t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cfyh601e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.35\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_071328-cfyh601e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/cfyh601e' target=\"_blank\">playful-sweep-1</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/hcp01g1t' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/hcp01g1t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/hcp01g1t' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/hcp01g1t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/cfyh601e' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/cfyh601e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.4911, Seq Acc = 0.2413, Token Acc = 0.6442 Test Seq Acc = 0.2410, Test Token Acc = 0.6395\n",
      "Epoch 2: Train Loss = 0.7016, Seq Acc = 0.3505, Token Acc = 0.7269 Test Seq Acc = 0.3606, Test Token Acc = 0.7331\n",
      "Epoch 3: Train Loss = 0.5182, Seq Acc = 0.3960, Token Acc = 0.7471 Test Seq Acc = 0.3995, Test Token Acc = 0.7535\n",
      "Epoch 4: Train Loss = 0.4284, Seq Acc = 0.4093, Token Acc = 0.7589 Test Seq Acc = 0.4075, Test Token Acc = 0.7639\n",
      "Epoch 5: Train Loss = 0.3630, Seq Acc = 0.4294, Token Acc = 0.7761 Test Seq Acc = 0.4326, Test Token Acc = 0.7800\n",
      "Epoch 6: Train Loss = 0.3174, Seq Acc = 0.4370, Token Acc = 0.7800 Test Seq Acc = 0.4285, Test Token Acc = 0.7800\n",
      "Epoch 7: Train Loss = 0.2829, Seq Acc = 0.4364, Token Acc = 0.7796 Test Seq Acc = 0.4421, Test Token Acc = 0.7858\n",
      "Epoch 8: Train Loss = 0.2562, Seq Acc = 0.4368, Token Acc = 0.7737 Test Seq Acc = 0.4401, Test Token Acc = 0.7835\n",
      "Epoch 9: Train Loss = 0.2330, Seq Acc = 0.4455, Token Acc = 0.7816 Test Seq Acc = 0.4453, Test Token Acc = 0.7866\n",
      "Epoch 10: Train Loss = 0.2100, Seq Acc = 0.4528, Token Acc = 0.7865 Test Seq Acc = 0.4483, Test Token Acc = 0.7925\n",
      "Epoch 11: Train Loss = 0.1950, Seq Acc = 0.4437, Token Acc = 0.7828 Test Seq Acc = 0.4553, Test Token Acc = 0.7914\n",
      "Epoch 12: Train Loss = 0.1787, Seq Acc = 0.4501, Token Acc = 0.7834 Test Seq Acc = 0.4513, Test Token Acc = 0.7863\n",
      "Epoch 13: Train Loss = 0.1714, Seq Acc = 0.4393, Token Acc = 0.7804 Test Seq Acc = 0.4465, Test Token Acc = 0.7930\n",
      "Epoch 14: Train Loss = 0.1549, Seq Acc = 0.4446, Token Acc = 0.7870 Test Seq Acc = 0.4538, Test Token Acc = 0.7910\n",
      "Epoch 15: Train Loss = 0.1476, Seq Acc = 0.4464, Token Acc = 0.7893 Test Seq Acc = 0.4471, Test Token Acc = 0.7937\n",
      "Epoch 16: Train Loss = 0.1394, Seq Acc = 0.4359, Token Acc = 0.7805 Test Seq Acc = 0.4449, Test Token Acc = 0.7916\n",
      "Epoch 17: Train Loss = 0.1321, Seq Acc = 0.4480, Token Acc = 0.7860 Test Seq Acc = 0.4496, Test Token Acc = 0.7959\n",
      "Epoch 18: Train Loss = 0.1266, Seq Acc = 0.4494, Token Acc = 0.7878 Test Seq Acc = 0.4515, Test Token Acc = 0.7927\n",
      "Epoch 19: Train Loss = 0.1195, Seq Acc = 0.4434, Token Acc = 0.7868 Test Seq Acc = 0.4520, Test Token Acc = 0.7941\n",
      "Epoch 20: Train Loss = 0.1128, Seq Acc = 0.4510, Token Acc = 0.7876 Test Seq Acc = 0.4392, Test Token Acc = 0.7901\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_34c67_row0_col0, #T_34c67_row0_col1, #T_34c67_row0_col2, #T_34c67_row0_col3, #T_34c67_row8_col0, #T_34c67_row8_col1, #T_34c67_row8_col2, #T_34c67_row8_col3, #T_34c67_row9_col0, #T_34c67_row9_col1, #T_34c67_row9_col2, #T_34c67_row9_col3, #T_34c67_row10_col0, #T_34c67_row10_col1, #T_34c67_row10_col2, #T_34c67_row10_col3, #T_34c67_row11_col0, #T_34c67_row11_col1, #T_34c67_row11_col2, #T_34c67_row11_col3, #T_34c67_row14_col0, #T_34c67_row14_col1, #T_34c67_row14_col2, #T_34c67_row14_col3 {\n",
       "  background-color: lightpink;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_34c67_row1_col0, #T_34c67_row1_col1, #T_34c67_row1_col2, #T_34c67_row1_col3, #T_34c67_row2_col0, #T_34c67_row2_col1, #T_34c67_row2_col2, #T_34c67_row2_col3, #T_34c67_row3_col0, #T_34c67_row3_col1, #T_34c67_row3_col2, #T_34c67_row3_col3, #T_34c67_row4_col0, #T_34c67_row4_col1, #T_34c67_row4_col2, #T_34c67_row4_col3, #T_34c67_row5_col0, #T_34c67_row5_col1, #T_34c67_row5_col2, #T_34c67_row5_col3, #T_34c67_row6_col0, #T_34c67_row6_col1, #T_34c67_row6_col2, #T_34c67_row6_col3, #T_34c67_row7_col0, #T_34c67_row7_col1, #T_34c67_row7_col2, #T_34c67_row7_col3, #T_34c67_row12_col0, #T_34c67_row12_col1, #T_34c67_row12_col2, #T_34c67_row12_col3, #T_34c67_row13_col0, #T_34c67_row13_col1, #T_34c67_row13_col2, #T_34c67_row13_col3 {\n",
       "  background-color: lightgreen;\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_34c67\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_34c67_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n",
       "      <th id=\"T_34c67_level0_col1\" class=\"col_heading level0 col1\" >Predicted</th>\n",
       "      <th id=\"T_34c67_level0_col2\" class=\"col_heading level0 col2\" >True</th>\n",
       "      <th id=\"T_34c67_level0_col3\" class=\"col_heading level0 col3\" >Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_34c67_row0_col0\" class=\"data row0 col0\" >amgathavavum</td>\n",
       "      <td id=\"T_34c67_row0_col1\" class=\"data row0 col1\" >അംഗതാവും</td>\n",
       "      <td id=\"T_34c67_row0_col2\" class=\"data row0 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_34c67_row0_col3\" class=\"data row0 col3\" >അംഗത<b style=\"color:red\">ാ</b>വ<b style=\"color:red\">ു</b><b style=\"color:red\">ം</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_34c67_row1_col0\" class=\"data row1 col0\" >amgathvavum</td>\n",
       "      <td id=\"T_34c67_row1_col1\" class=\"data row1 col1\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_34c67_row1_col2\" class=\"data row1 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_34c67_row1_col3\" class=\"data row1 col3\" >അംഗത്വവും</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_34c67_row2_col0\" class=\"data row2 col0\" >angathwavum</td>\n",
       "      <td id=\"T_34c67_row2_col1\" class=\"data row2 col1\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_34c67_row2_col2\" class=\"data row2 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_34c67_row2_col3\" class=\"data row2 col3\" >അംഗത്വവും</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_34c67_row3_col0\" class=\"data row3 col0\" >amgabalam</td>\n",
       "      <td id=\"T_34c67_row3_col1\" class=\"data row3 col1\" >അംഗബലം</td>\n",
       "      <td id=\"T_34c67_row3_col2\" class=\"data row3 col2\" >അംഗബലം</td>\n",
       "      <td id=\"T_34c67_row3_col3\" class=\"data row3 col3\" >അംഗബലം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_34c67_row4_col0\" class=\"data row4 col0\" >angabalam</td>\n",
       "      <td id=\"T_34c67_row4_col1\" class=\"data row4 col1\" >അംഗബലം</td>\n",
       "      <td id=\"T_34c67_row4_col2\" class=\"data row4 col2\" >അംഗബലം</td>\n",
       "      <td id=\"T_34c67_row4_col3\" class=\"data row4 col3\" >അംഗബലം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_34c67_row5_col0\" class=\"data row5 col0\" >amgeekarikkuka</td>\n",
       "      <td id=\"T_34c67_row5_col1\" class=\"data row5 col1\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_34c67_row5_col2\" class=\"data row5 col2\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_34c67_row5_col3\" class=\"data row5 col3\" >അംഗീകരിക്കുക</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_34c67_row6_col0\" class=\"data row6 col0\" >angeekarikkuka</td>\n",
       "      <td id=\"T_34c67_row6_col1\" class=\"data row6 col1\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_34c67_row6_col2\" class=\"data row6 col2\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_34c67_row6_col3\" class=\"data row6 col3\" >അംഗീകരിക്കുക</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_34c67_row7_col0\" class=\"data row7 col0\" >ambaasadar</td>\n",
       "      <td id=\"T_34c67_row7_col1\" class=\"data row7 col1\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_34c67_row7_col2\" class=\"data row7 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_34c67_row7_col3\" class=\"data row7 col3\" >അംബാസഡർ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_34c67_row8_col0\" class=\"data row8 col0\" >ambaassador</td>\n",
       "      <td id=\"T_34c67_row8_col1\" class=\"data row8 col1\" >അൻബാസ്സർ</td>\n",
       "      <td id=\"T_34c67_row8_col2\" class=\"data row8 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_34c67_row8_col3\" class=\"data row8 col3\" >അ<b style=\"color:red\">ൻ</b>ബാസ<b style=\"color:red\">്</b><b style=\"color:red\">സ</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_34c67_row9_col0\" class=\"data row9 col0\" >ambassador</td>\n",
       "      <td id=\"T_34c67_row9_col1\" class=\"data row9 col1\" >അൻബസ്സാർ</td>\n",
       "      <td id=\"T_34c67_row9_col2\" class=\"data row9 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_34c67_row9_col3\" class=\"data row9 col3\" >അ<b style=\"color:red\">ൻ</b>ബ<b style=\"color:red\">സ</b><b style=\"color:red\">്</b><b style=\"color:red\">സ</b><b style=\"color:red\">ാ</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_34c67_row10_col0\" class=\"data row10 col0\" >akathekku</td>\n",
       "      <td id=\"T_34c67_row10_col1\" class=\"data row10 col1\" >അകത്തേക്ക്</td>\n",
       "      <td id=\"T_34c67_row10_col2\" class=\"data row10 col2\" >അകത്തേക്കു</td>\n",
       "      <td id=\"T_34c67_row10_col3\" class=\"data row10 col3\" >അകത്തേക്ക<b style=\"color:red\">്</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_34c67_row11_col0\" class=\"data row11 col0\" >akkitham</td>\n",
       "      <td id=\"T_34c67_row11_col1\" class=\"data row11 col1\" >അക്കിതം</td>\n",
       "      <td id=\"T_34c67_row11_col2\" class=\"data row11 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_34c67_row11_col3\" class=\"data row11 col3\" >അക്കിത<b style=\"color:red\">ം</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_34c67_row12_col0\" class=\"data row12 col0\" >akkithham</td>\n",
       "      <td id=\"T_34c67_row12_col1\" class=\"data row12 col1\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_34c67_row12_col2\" class=\"data row12 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_34c67_row12_col3\" class=\"data row12 col3\" >അക്കിത്തം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_34c67_row13_col0\" class=\"data row13 col0\" >akkittham</td>\n",
       "      <td id=\"T_34c67_row13_col1\" class=\"data row13 col1\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_34c67_row13_col2\" class=\"data row13 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_34c67_row13_col3\" class=\"data row13 col3\" >അക്കിത്തം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_34c67_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_34c67_row14_col0\" class=\"data row14 col0\" >accountil</td>\n",
       "      <td id=\"T_34c67_row14_col1\" class=\"data row14 col1\" >അക്കയന്റിൽ</td>\n",
       "      <td id=\"T_34c67_row14_col2\" class=\"data row14 col2\" >അക്കൗണ്ടിൽ</td>\n",
       "      <td id=\"T_34c67_row14_col3\" class=\"data row14 col3\" >അക്ക<b style=\"color:red\">യ</b><b style=\"color:red\">ന</b>്<b style=\"color:red\">റ</b>ിൽ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to predictions_vanilla/test_predictions_embed256_hid128_enc2_dec1_LSTM_drop0.35_beam5.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>test_accuracy</td><td>▁▅▆▆▇▇█████████████▇</td></tr><tr><td>test_token_accuracy</td><td>▁▅▆▇▇▇█▇████████████</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▆▇▇▇▇▇███████▇████</td></tr><tr><td>val_token_accuracy</td><td>▁▅▆▇▇██▇████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>test_accuracy</td><td>0.43922</td></tr><tr><td>test_token_accuracy</td><td>0.79014</td></tr><tr><td>train_loss</td><td>0.11281</td></tr><tr><td>val_accuracy</td><td>0.45098</td></tr><tr><td>val_token_accuracy</td><td>0.78757</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed256_hid128_enc2_dec1_LSTM_drop0.35_beam5</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/cfyh601e' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/cfyh601e</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_071328-cfyh601e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip created: predictions_vanilla.zip\n"
     ]
    }
   ],
   "source": [
    "## import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import zipfile\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Char-level vocabulary\n",
    "class CharVocab:\n",
    "     \"\"\"\n",
    "    A character-level vocabulary class for encoding and decoding sequences of characters.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    char2idx : dict\n",
    "        Mapping from characters to integer indices. Includes special tokens:\n",
    "        '<pad>' (0), '<sos>' (1), '<eos>' (2), and '<unk>' (3).\n",
    "        \n",
    "    idx2char : dict\n",
    "        Reverse mapping from indices to characters.\n",
    "\n",
    "    pad_idx : int\n",
    "        Index of the padding token ('<pad>').\n",
    "\n",
    "    sos_idx : int\n",
    "        Index of the start-of-sequence token ('<sos>').\n",
    "\n",
    "    eos_idx : int\n",
    "        Index of the end-of-sequence token ('<eos>').\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    encode(word: str) -> List[int]\n",
    "        Converts a string into a list of indices, including <sos> at the start and <eos> at the end.\n",
    "        Unknown characters are mapped to the <unk> index.\n",
    "\n",
    "    decode(ids: List[int]) -> str\n",
    "        Converts a list of indices back into a string, ignoring <sos> and <pad>, and stopping at <eos>.\n",
    "\n",
    "    __len__() -> int\n",
    "        Returns the size of the vocabulary (i.e., number of unique tokens including special tokens).\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        chars = sorted(set(\"\".join(words)))\n",
    "        self.char2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        for c in chars:\n",
    "            self.char2idx[c] = len(self.char2idx)\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.pad_idx = self.char2idx['<pad>']\n",
    "        self.sos_idx = self.char2idx['<sos>']\n",
    "        self.eos_idx = self.char2idx['<eos>']\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.sos_idx] + [self.char2idx.get(c, self.char2idx['<unk>']) for c in word] + [self.eos_idx]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx == self.eos_idx:\n",
    "                break\n",
    "            if idx not in (self.sos_idx, self.pad_idx):\n",
    "                chars.append(self.idx2char.get(idx, ''))\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "# Load data\n",
    "def read_file(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if len(line.split('\\t')) >= 2]\n",
    "\n",
    "train_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.train.tsv')\n",
    "dev_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.dev.tsv')\n",
    "test_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.test.tsv')\n",
    "\n",
    "# Build vocabularies from ALL data to ensure full coverage\n",
    "src_vocab = CharVocab([src for _, src in train_pairs])\n",
    "tgt_vocab = CharVocab([tgt for tgt, _ in train_pairs])\n",
    "\n",
    "\n",
    "# Dataset\n",
    "class TransliterationDataset(Dataset):\n",
    "    \"\"\"\n",
    "This code defines a PyTorch dataset and dataloaders for a character-level transliteration task. It converts input-output string pairs into sequences of token indices using source and target vocabularies, and pads them for batch processing. The TransliterationDataset encodes each word pair, while the collate_fn ensures proper padding during batching. Dataloaders are created for training and validation with appropriate batch sizes.\n",
    "\"\"\"\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.data = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt, src = self.data[idx]\n",
    "        return torch.tensor(self.src_vocab.encode(src)), torch.tensor(self.tgt_vocab.encode(tgt))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "train_loader = DataLoader(TransliterationDataset(train_pairs, src_vocab, tgt_vocab),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(TransliterationDataset(dev_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(TransliterationDataset(test_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_enc_layers = config.enc_layers\n",
    "        self.num_dec_layers = config.dec_layers\n",
    "        self.cell_type = config.cell\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.max_len = 30\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, self.embedding_dim)\n",
    "\n",
    "        RNN = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]\n",
    "        self.encoder = RNN(self.embedding_dim, self.hidden_size, num_layers=self.num_enc_layers,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.decoder = RNN(self.embedding_dim, self.hidden_size * 2, num_layers=self.num_dec_layers,\n",
    "                           batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(self.hidden_size * 2, output_vocab_size)\n",
    "\n",
    "        self.sos_idx = tgt_vocab.sos_idx\n",
    "        self.eos_idx = tgt_vocab.eos_idx\n",
    "        self.pad_idx = tgt_vocab.pad_idx\n",
    "\n",
    "    def encode(self, src):\n",
    "        embedded = self.dropout(self.encoder_embedding(src))\n",
    "        outputs, h_n = self.encoder(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h, c = h_n\n",
    "            h_cat = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)\n",
    "            c_cat = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, (h_cat, c_cat)\n",
    "        else:\n",
    "            h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, h_cat\n",
    "\n",
    "    def decode_step(self, input_token, hidden):\n",
    "        embedded = self.dropout(self.decoder_embedding(input_token))\n",
    "        output, hidden = self.decoder(embedded, hidden)\n",
    "        logits = self.fc(output.squeeze(1))\n",
    "        return logits, hidden\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        _, hidden = self.encode(src)\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden = self.decode_step(input_token, hidden)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation\n",
    "def beam_decode(model, src, beam_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        _, hidden = model.encode(src)\n",
    "        batch_size = src.size(0)\n",
    "        final_outputs = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            h_b = (hidden[0][:, b:b+1, :].contiguous(), hidden[1][:, b:b+1, :].contiguous()) if model.cell_type == 'LSTM' else hidden[:, b:b+1, :].contiguous()\n",
    "            beams = [([model.sos_idx], 0.0, h_b)]\n",
    "            for _ in range(model.max_len):\n",
    "                new_beams = []\n",
    "                for seq, score, h in beams:\n",
    "                    if seq[-1] == model.eos_idx:\n",
    "                        new_beams.append((seq, score, h))\n",
    "                        continue\n",
    "                    input_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                    out, h_new = model.decode_step(input_token, h)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    topk_probs, topk_idxs = torch.topk(log_probs, beam_size, dim=1)\n",
    "                    for i in range(beam_size):\n",
    "                        next_seq = seq + [topk_idxs[0][i].item()]\n",
    "                        new_score = score + topk_probs[0][i].item()\n",
    "                        new_beams.append((next_seq, new_score, h_new))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            final_outputs.append(beams[0][0])\n",
    "        return final_outputs\n",
    "\n",
    "def evaluate_beam(model, dataloader, beam_size):\n",
    "    model.eval()\n",
    "    total_seq, correct_seq = 0, 0\n",
    "    total_tokens, correct_tokens = 0, 0\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            preds = beam_decode(model, src, beam_size)\n",
    "            \n",
    "            # Decode each item in the batch individually\n",
    "            for i, (pred, true) in enumerate(zip(preds, tgt)):\n",
    "                # Process predicted sequence\n",
    "                pred_trimmed = [tok for tok in pred[1:] if tok != model.pad_idx and tok != model.eos_idx]\n",
    "                # Process true sequence\n",
    "                true_trimmed = [tok.item() for tok in true[1:] if tok.item() != model.pad_idx and tok.item() != model.eos_idx]\n",
    "\n",
    "                # Sequence-level accuracy\n",
    "                if pred_trimmed == true_trimmed:\n",
    "                    correct_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "                # Token-level accuracy\n",
    "                for p, t in zip(pred_trimmed, true_trimmed):\n",
    "                    if p == t:\n",
    "                        correct_tokens += 1\n",
    "                total_tokens += len(true_trimmed)\n",
    "                \n",
    "                # Decode source, predicted and true words properly\n",
    "                src_word = src_vocab.decode([x.item() for x in src[i] if x.item() not in (src_vocab.sos_idx, src_vocab.eos_idx, src_vocab.pad_idx)])\n",
    "                pred_word = tgt_vocab.decode(pred)\n",
    "                true_word = tgt_vocab.decode([x.item() for x in true if x.item() not in (tgt_vocab.pad_idx, tgt_vocab.eos_idx)])\n",
    "                \n",
    "                all_predictions.append((src_word, pred_word, true_word))\n",
    "\n",
    "    seq_accuracy = correct_seq / total_seq if total_seq > 0 else 0.0\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    return seq_accuracy, token_accuracy, all_predictions\n",
    "\n",
    "def visualize_predictions(predictions, num_samples=10, log_to_wandb=False):\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(predictions[:num_samples], columns=['Input', 'Predicted', 'True'])\n",
    "    \n",
    "    # Highlight differences between Predicted and True\n",
    "    def highlight_diff(row):\n",
    "        pred, true = row['Predicted'], row['True']\n",
    "        diff = []\n",
    "        for p, t in zip(pred, true):\n",
    "            if p == t:\n",
    "                diff.append(p)\n",
    "            else:\n",
    "                diff.append(f'<b style=\"color:red\">{p}</b>')  # Highlight incorrect chars in red\n",
    "        return ''.join(diff)\n",
    "    \n",
    "    # Add a \"Difference\" column\n",
    "    df['Difference'] = df.apply(lambda row: highlight_diff(row), axis=1)\n",
    "    \n",
    "    # Color entire row green if correct, else pink\n",
    "    def row_style(row):\n",
    "        color = 'lightgreen' if row['Predicted'] == row['True'] else 'lightpink'\n",
    "        return [f'background-color: {color}' for _ in row]\n",
    "    \n",
    "    # Apply styling\n",
    "    styled_df = df.style.apply(row_style, axis=1).set_properties(**{'text-align': 'left'})\n",
    "    \n",
    "    # Display in Jupyter (HTML)\n",
    "    display(HTML(styled_df.to_html(escape=False)))\n",
    "    \n",
    "    # Log to W&B (if enabled)\n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"predictions\": wandb.Table(dataframe=df)})\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    os.makedirs('predictions_vanilla', exist_ok=True)\n",
    "    df = pd.DataFrame(predictions, columns=['English Input', 'Predicted Native', 'True Native'])\n",
    "    df.to_csv(f'predictions_vanilla/{filename}', index=False)\n",
    "    print(f\"Saved to predictions_vanilla/{filename}\")\n",
    "\n",
    "# W&B Training\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        run_name = f\"embed{config.embed_size}_hid{config.hidden_size}_enc{config.enc_layers}_dec{config.dec_layers}_{config.cell}_drop{config.dropout}_beam{config.beam_size}\"\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        model = Seq2Seq(config, len(src_vocab), len(tgt_vocab)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "\n",
    "        for epoch in range(20):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion)\n",
    "            acc, token_acc, val_preds = evaluate_beam(model, dev_loader, beam_size=config.beam_size)\n",
    "            test_acc, test_token_acc, test_preds = evaluate_beam(model, test_loader, beam_size=config.beam_size)\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_accuracy': acc,\n",
    "                'val_token_accuracy': token_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'test_token_accuracy': test_token_acc\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Seq Acc = {acc:.4f}, Token Acc = {token_acc:.4f} Test Seq Acc = {test_acc:.4f}, Test Token Acc = {test_token_acc:.4f}\")\n",
    "        visualize_predictions(test_preds, num_samples=15, log_to_wandb=True)\n",
    "        save_predictions(test_preds, f'test_predictions_{run_name}.csv')\n",
    "\n",
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [256]},\n",
    "        'hidden_size': {'values': [128]},\n",
    "        'enc_layers': {'values': [2]},\n",
    "        'dec_layers': {'values': [1]},\n",
    "        'dropout': {'values': [0.35]},\n",
    "        'cell': {'values': ['LSTM']},\n",
    "        'beam_size': {'values': [5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"A3_ce21b020\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=1)\n",
    "\n",
    "# Create zip\n",
    "def create_prediction_zip():\n",
    "    with zipfile.ZipFile('predictions_vanilla.zip', 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('predictions_vanilla'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "    print(\"Zip created: predictions_vanilla.zip\")\n",
    "\n",
    "create_prediction_zip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best model using attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-05-20T12:56:24.562Z",
     "iopub.execute_input": "2025-05-20T10:36:10.451009Z",
     "iopub.status.busy": "2025-05-20T10:36:10.450680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: vgrgvz79\n",
      "Sweep URL: https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/sweeps/vgrgvz79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: n8rd5zr1 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_103625-n8rd5zr1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/runs/n8rd5zr1' target=\"_blank\">solar-sweep-1</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/sweeps/vgrgvz79' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/sweeps/vgrgvz79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/sweeps/vgrgvz79' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/sweeps/vgrgvz79</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/runs/n8rd5zr1' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_attention_ce21b020/runs/n8rd5zr1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.0981, Val Acc = 0.4072, Test Acc = 0.4125\n",
      "Epoch 2: Train Loss = 0.4345, Val Acc = 0.4583, Test Acc = 0.4506\n",
      "Epoch 3: Train Loss = 0.3356, Val Acc = 0.4856, Test Acc = 0.4848\n",
      "Epoch 4: Train Loss = 0.2783, Val Acc = 0.4866, Test Acc = 0.4824\n",
      "Epoch 5: Train Loss = 0.2367, Val Acc = 0.5054, Test Acc = 0.4920\n",
      "Epoch 6: Train Loss = 0.2121, Val Acc = 0.5136, Test Acc = 0.5123\n",
      "Epoch 7: Train Loss = 0.1827, Val Acc = 0.5155, Test Acc = 0.5242\n",
      "Epoch 8: Train Loss = 0.1695, Val Acc = 0.5063, Test Acc = 0.5155\n",
      "Epoch 9: Train Loss = 0.1513, Val Acc = 0.5276, Test Acc = 0.5127\n",
      "Epoch 10: Train Loss = 0.1402, Val Acc = 0.5022, Test Acc = 0.5052\n",
      "Epoch 11: Train Loss = 0.1327, Val Acc = 0.5169, Test Acc = 0.5194\n",
      "Epoch 12: Train Loss = 0.1174, Val Acc = 0.5139, Test Acc = 0.5185\n",
      "Epoch 13: Train Loss = 0.1116, Val Acc = 0.5102, Test Acc = 0.5082\n",
      "Epoch 14: Train Loss = 0.1068, Val Acc = 0.5153, Test Acc = 0.5169\n",
      "Epoch 15: Train Loss = 0.1019, Val Acc = 0.5178, Test Acc = 0.5134\n",
      "Epoch 16: Train Loss = 0.0967, Val Acc = 0.5267, Test Acc = 0.5228\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import zipfile\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Char-level vocabulary\n",
    "class CharVocab:\n",
    "     \"\"\"\n",
    "    A character-level vocabulary class for encoding and decoding sequences of characters.\n",
    "\n",
    "    Attributes:\n",
    "    -----------\n",
    "    char2idx : dict\n",
    "        Mapping from characters to integer indices. Includes special tokens:\n",
    "        '<pad>' (0), '<sos>' (1), '<eos>' (2), and '<unk>' (3).\n",
    "        \n",
    "    idx2char : dict\n",
    "        Reverse mapping from indices to characters.\n",
    "\n",
    "    pad_idx : int\n",
    "        Index of the padding token ('<pad>').\n",
    "\n",
    "    sos_idx : int\n",
    "        Index of the start-of-sequence token ('<sos>').\n",
    "\n",
    "    eos_idx : int\n",
    "        Index of the end-of-sequence token ('<eos>').\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "    encode(word: str) -> List[int]\n",
    "        Converts a string into a list of indices, including <sos> at the start and <eos> at the end.\n",
    "        Unknown characters are mapped to the <unk> index.\n",
    "\n",
    "    decode(ids: List[int]) -> str\n",
    "        Converts a list of indices back into a string, ignoring <sos> and <pad>, and stopping at <eos>.\n",
    "\n",
    "    __len__() -> int\n",
    "        Returns the size of the vocabulary (i.e., number of unique tokens including special tokens).\n",
    "    \"\"\"\n",
    "    def __init__(self, words):\n",
    "        chars = sorted(set(\"\".join(words)))\n",
    "        self.char2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        for c in chars:\n",
    "            self.char2idx[c] = len(self.char2idx)\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.pad_idx = self.char2idx['<pad>']\n",
    "        self.sos_idx = self.char2idx['<sos>']\n",
    "        self.eos_idx = self.char2idx['<eos>']\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.sos_idx] + [self.char2idx.get(c, self.char2idx['<unk>']) for c in word] + [self.eos_idx]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx == self.eos_idx:\n",
    "                break\n",
    "            if idx not in (self.sos_idx, self.pad_idx):\n",
    "                chars.append(self.idx2char.get(idx, ''))\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if len(line.split('\\t')) >= 2]\n",
    "\n",
    "train_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.train.tsv')\n",
    "dev_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.dev.tsv')\n",
    "test_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.test.tsv')\n",
    "\n",
    "src_vocab = CharVocab([src for _, src in train_pairs])\n",
    "tgt_vocab = CharVocab([tgt for tgt, _ in train_pairs])\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.data = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt, src = self.data[idx]\n",
    "        return torch.tensor(self.src_vocab.encode(src)), torch.tensor(self.tgt_vocab.encode(tgt))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "train_loader = DataLoader(TransliterationDataset(train_pairs, src_vocab, tgt_vocab),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(TransliterationDataset(dev_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(TransliterationDataset(test_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_enc_layers = config.enc_layers\n",
    "        self.num_dec_layers = config.dec_layers\n",
    "        self.cell_type = config.cell\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.max_len = 30\n",
    "        self.attention_weights = []  # Store attention weights for visualization\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, self.embedding_dim)\n",
    "\n",
    "        RNN = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]\n",
    "        self.encoder = RNN(self.embedding_dim, self.hidden_size, num_layers=self.num_enc_layers,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.decoder = RNN(self.embedding_dim + self.hidden_size * 2, self.hidden_size * 2,\n",
    "                           num_layers=self.num_dec_layers, batch_first=True)\n",
    "\n",
    "        self.attention = Attention(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        self.fc = nn.Linear(self.hidden_size * 4, output_vocab_size)\n",
    "\n",
    "        self.sos_idx = tgt_vocab.sos_idx\n",
    "        self.eos_idx = tgt_vocab.eos_idx\n",
    "        self.pad_idx = tgt_vocab.pad_idx\n",
    "\n",
    "    def encode(self, src):\n",
    "        embedded = self.dropout(self.encoder_embedding(src))\n",
    "        outputs, h_n = self.encoder(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h, c = h_n\n",
    "            h_cat = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)\n",
    "            c_cat = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, (h_cat, c_cat)\n",
    "        else:\n",
    "            h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, h_cat\n",
    "\n",
    "    def decode_step(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.decoder_embedding(input_token))\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h_t = hidden[0][-1]\n",
    "        else:\n",
    "            h_t = hidden[-1]\n",
    "            \n",
    "        attn_weights = self.attention(h_t, encoder_outputs)\n",
    "        self.attention_weights.append(attn_weights)  # Store for visualization\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.decoder(rnn_input, hidden)\n",
    "        logits = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return logits, hidden, attn_weights\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        self.attention_weights = []  # Reset attention weights storage\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        encoder_outputs, hidden = self.encode(src)\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, attn_weights = self.decode_step(input_token, hidden, encoder_outputs)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def beam_decode(model, src, beam_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encode(src)\n",
    "        batch_size = src.size(0)\n",
    "        final_outputs = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            h_b = (hidden[0][:, b:b+1, :].contiguous(), hidden[1][:, b:b+1, :].contiguous()) if model.cell_type == 'LSTM' else hidden[:, b:b+1, :].contiguous()\n",
    "            enc_out_b = encoder_outputs[b:b+1]\n",
    "            beams = [([model.sos_idx], 0.0, h_b)]\n",
    "            \n",
    "            for _ in range(model.max_len):\n",
    "                new_beams = []\n",
    "                for seq, score, h in beams:\n",
    "                    if seq[-1] == model.eos_idx:\n",
    "                        new_beams.append((seq, score, h))\n",
    "                        continue\n",
    "                    input_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                    out, h_new, _ = model.decode_step(input_token, h, enc_out_b)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    topk_probs, topk_idxs = torch.topk(log_probs, beam_size, dim=1)\n",
    "                    for i in range(beam_size):\n",
    "                        next_seq = seq + [topk_idxs[0][i].item()]\n",
    "                        new_score = score + topk_probs[0][i].item()\n",
    "                        new_beams.append((next_seq, new_score, h_new))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            final_outputs.append(beams[0][0])\n",
    "        return final_outputs\n",
    "\n",
    "def plot_attention_heatmaps(model, test_samples, num_samples=9):\n",
    "    model.eval()\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    samples = test_samples[:num_samples]\n",
    "    \n",
    "    for i, (src, tgt) in enumerate(samples):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        with torch.no_grad():\n",
    "            src_tensor = torch.tensor([src_vocab.encode(src)], device=device)\n",
    "            tgt_tensor = torch.tensor([tgt_vocab.encode(tgt)], device=device)\n",
    "            model(src_tensor, tgt_tensor)  # This populates attention_weights\n",
    "            \n",
    "            # Get attention weights and convert to numpy\n",
    "            attn_weights = torch.cat(model.attention_weights).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Create heatmap\n",
    "            sns.heatmap(attn_weights, cmap=\"YlGnBu\", \n",
    "                        xticklabels=list(src),\n",
    "                        yticklabels=list(tgt_vocab.decode(tgt_tensor[0][1:-1])))\n",
    "            plt.title(f\"Input: {src}\\nOutput: {tgt_vocab.decode(tgt_tensor[0][1:-1])}\")\n",
    "            plt.xlabel(\"Source Characters\")\n",
    "            plt.ylabel(\"Target Characters\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def evaluate_beam(model, dataloader, beam_size):\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "    total_seq, correct_seq = 0, 0  # Track full sequence accuracy\n",
    "    total_tokens, correct_tokens = 0, 0  # Track token-level accuracy\n",
    "    all_predictions = []  # Store predictions for analysis/visualization\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation for faster evaluation\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)  # Move tensors to device (CPU/GPU)\n",
    "\n",
    "            preds = beam_decode(model, src, beam_size)  # Perform beam search decoding\n",
    "\n",
    "            for i, (pred, true) in enumerate(zip(preds, tgt)):\n",
    "                # Remove special tokens (pad/eos) from predictions and targets\n",
    "                pred_trimmed = [tok for tok in pred[1:] if tok != model.pad_idx and tok != model.eos_idx]\n",
    "                true_trimmed = [tok.item() for tok in true[1:] if tok.item() != model.pad_idx and tok.item() != model.eos_idx]\n",
    "\n",
    "                # Check if entire sequence matches exactly\n",
    "                if pred_trimmed == true_trimmed:\n",
    "                    correct_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "                # Compare token-by-token for token-level accuracy\n",
    "                for p, t in zip(pred_trimmed, true_trimmed):\n",
    "                    if p == t:\n",
    "                        correct_tokens += 1\n",
    "                total_tokens += len(true_trimmed)\n",
    "\n",
    "                # Decode input, prediction, and ground truth for storage\n",
    "                src_word = src_vocab.decode([x.item() for x in src[i] if x.item() not in (src_vocab.sos_idx, src_vocab.eos_idx, src_vocab.pad_idx)])\n",
    "                pred_word = tgt_vocab.decode(pred)\n",
    "                true_word = tgt_vocab.decode([x.item() for x in true if x.item() not in (tgt_vocab.pad_idx, tgt_vocab.eos_idx)])\n",
    "\n",
    "                # Store triplet: (input word, predicted word, true word)\n",
    "                all_predictions.append((src_word, pred_word, true_word))\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    seq_accuracy = correct_seq / total_seq if total_seq > 0 else 0.0\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "\n",
    "    return seq_accuracy, token_accuracy, all_predictions  # Return both accuracy metrics and all decoded predictions\n",
    "\n",
    "def visualize_predictions(predictions, num_samples=10, log_to_wandb=False):\n",
    "    \"\"\"\n",
    "    Visualizes a subset of model predictions by highlighting character-level differences \n",
    "    between the predicted and true output.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    predictions : list of tuples or lists\n",
    "        Each element should be a (input, predicted, true) triple representing the input string,\n",
    "        the model's prediction, and the ground truth string.\n",
    "\n",
    "    num_samples : int, optional (default=10)\n",
    "        The number of prediction samples to display.\n",
    "\n",
    "    log_to_wandb : bool, optional (default=False)\n",
    "        If True, logs the raw predictions to Weights & Biases using wandb.Table.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    styled_df\n",
    "\n",
    "    Notes:\n",
    "    ------\n",
    "    - This function is useful for qualitative evaluation of sequence prediction models.\n",
    "    - It uses HTML and pandas styling to visually compare predictions.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(predictions[:num_samples], columns=['Input', 'Predicted', 'True'])\n",
    "    \n",
    "    def highlight_diff(row):\n",
    "        pred, true = row['Predicted'], row['True']\n",
    "        diff = []\n",
    "        for p, t in zip(pred, true):\n",
    "            if p == t:\n",
    "                diff.append(p)\n",
    "            else:\n",
    "                diff.append(f'<b style=\"color:red\">{p}</b>')\n",
    "        return ''.join(diff)\n",
    "    \n",
    "    df['Difference'] = df.apply(lambda row: highlight_diff(row), axis=1)\n",
    "    \n",
    "    def row_style(row):\n",
    "        color = 'lightgreen' if row['Predicted'] == row['True'] else 'lightpink'\n",
    "        return [f'background-color: {color}' for _ in row]\n",
    "    \n",
    "    styled_df = df.style.apply(row_style, axis=1).set_properties(**{'text-align': 'left'})\n",
    "    display(HTML(styled_df.to_html(escape=False)))\n",
    "    \n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"predictions\": wandb.Table(dataframe=df)})\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    os.makedirs('predictions_attention', exist_ok=True)\n",
    "    df = pd.DataFrame(predictions, columns=['Input', 'Predicted', 'True'])\n",
    "    df.to_csv(f'predictions_attention/{filename}', index=False)\n",
    "    print(f\"Saved to predictions_attention/{filename}\")\n",
    "\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        run_name = f\"embed{config.embed_size}_hid{config.hidden_size}_enc{config.enc_layers}_dec{config.dec_layers}_{config.cell}_drop{config.dropout}_beam{config.beam_size}_attn\"\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        model = Seq2Seq(config, len(src_vocab), len(tgt_vocab)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "\n",
    "        for epoch in range(20):  # Increased epochs for better training\n",
    "            train_loss = train(model, train_loader, optimizer, criterion)\n",
    "            acc, token_acc, val_preds = evaluate_beam(model, dev_loader, beam_size=config.beam_size)\n",
    "            test_acc, test_token_acc, test_preds = evaluate_beam(model, test_loader, beam_size=config.beam_size)\n",
    "\n",
    "            # Log attention heatmaps at the end of training\n",
    "            if epoch == 19:\n",
    "                # Get first 9 test samples properly\n",
    "                test_samples = []\n",
    "                for i in range(9):\n",
    "                    src, tgt = test_loader.dataset[i]\n",
    "                    src_word = src_vocab.decode([x.item() for x in src if x.item() not in (src_vocab.sos_idx, src_vocab.eos_idx, src_vocab.pad_idx)])\n",
    "                    tgt_word = tgt_vocab.decode([x.item() for x in tgt if x.item() not in (tgt_vocab.pad_idx, tgt_vocab.eos_idx)])\n",
    "                    test_samples.append((src_word, tgt_word))\n",
    "                \n",
    "                attention_plot = plot_attention_heatmaps(model, test_samples)\n",
    "                wandb.log({\"attention_heatmaps\": wandb.Image(attention_plot)})\n",
    "                plt.close()\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_accuracy': acc,\n",
    "                'val_token_accuracy': token_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'test_token_accuracy': test_token_acc,\n",
    "                'used_attention': True\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Acc = {acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "        visualize_predictions(test_preds, num_samples=15, log_to_wandb=True)\n",
    "        save_predictions(test_preds, f'test_predictions_{run_name}.csv')\n",
    "\n",
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [256]},\n",
    "        'hidden_size': {'values': [128]},\n",
    "        'enc_layers': {'values': [2]},\n",
    "        'dec_layers': {'values': [1]},\n",
    "        'dropout': {'values': [0.30]},\n",
    "        'cell': {'values': ['LSTM']},\n",
    "        'beam_size': {'values': [5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"A3_attention_ce21b020\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=1)\n",
    "\n",
    "def create_prediction_zip():\n",
    "    with zipfile.ZipFile('predictions_attention.zip', 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('predictions_attention'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "    print(\"Zip created: predictions_attention.zip\")\n",
    "\n",
    "create_prediction_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-20T18:32:58.862998Z",
     "iopub.status.busy": "2025-05-20T18:32:58.862435Z",
     "iopub.status.idle": "2025-05-20T19:15:42.897882Z",
     "shell.execute_reply": "2025-05-20T19:15:42.897290Z",
     "shell.execute_reply.started": "2025-05-20T18:32:58.862976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 28jgsziy\n",
      "Sweep URL: https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/28jgsziy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xhz3l7hg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250520_183307-xhz3l7hg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/xhz3l7hg' target=\"_blank\">resilient-sweep-1</a></strong> to <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/28jgsziy' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/28jgsziy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/28jgsziy' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/sweeps/28jgsziy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/xhz3l7hg' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/xhz3l7hg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 1.0905, Val Acc = 0.3845, Test Acc = 0.3747\n",
      "Epoch 2: Train Loss = 0.4411, Val Acc = 0.4653, Test Acc = 0.4574\n",
      "Epoch 3: Train Loss = 0.3372, Val Acc = 0.4930, Test Acc = 0.4829\n",
      "Epoch 4: Train Loss = 0.2761, Val Acc = 0.5123, Test Acc = 0.5094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3392 (\\N{MALAYALAM VOWEL SIGN II}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3349 (\\N{MALAYALAM LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3376 (\\N{MALAYALAM LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3391 (\\N{MALAYALAM VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3392 (\\N{MALAYALAM VOWEL SIGN II}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3349 (\\N{MALAYALAM LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3376 (\\N{MALAYALAM LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3391 (\\N{MALAYALAM VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3392 (\\N{MALAYALAM VOWEL SIGN II}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3349 (\\N{MALAYALAM LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3376 (\\N{MALAYALAM LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3391 (\\N{MALAYALAM VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3390 (\\N{MALAYALAM VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3384 (\\N{MALAYALAM LETTER SA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3361 (\\N{MALAYALAM LETTER DDA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3452 (\\N{MALAYALAM LETTER CHILLU RR}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3392 (\\N{MALAYALAM VOWEL SIGN II}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3349 (\\N{MALAYALAM LETTER KA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3376 (\\N{MALAYALAM LETTER RA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3391 (\\N{MALAYALAM VOWEL SIGN I}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3390 (\\N{MALAYALAM VOWEL SIGN AA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3384 (\\N{MALAYALAM LETTER SA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3361 (\\N{MALAYALAM LETTER DDA}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/seaborn/utils.py:80: UserWarning: Glyph 3452 (\\N{MALAYALAM LETTER CHILLU RR}) missing from current font.\n",
      "  fig.canvas.draw()\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3333 (\\N{MALAYALAM LETTER A}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Matplotlib currently does not support Malayalam natively.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3330 (\\N{MALAYALAM SIGN ANUSVARA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3351 (\\N{MALAYALAM LETTER GA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3364 (\\N{MALAYALAM LETTER TA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3405 (\\N{MALAYALAM SIGN VIRAMA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3381 (\\N{MALAYALAM LETTER VA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3393 (\\N{MALAYALAM VOWEL SIGN U}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3372 (\\N{MALAYALAM LETTER BA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3378 (\\N{MALAYALAM LETTER LA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3392 (\\N{MALAYALAM VOWEL SIGN II}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3349 (\\N{MALAYALAM LETTER KA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3376 (\\N{MALAYALAM LETTER RA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3391 (\\N{MALAYALAM VOWEL SIGN I}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3390 (\\N{MALAYALAM VOWEL SIGN AA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3384 (\\N{MALAYALAM LETTER SA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3361 (\\N{MALAYALAM LETTER DDA}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n",
      "/usr/local/lib/python3.11/dist-packages/wandb/sdk/data_types/image.py:307: UserWarning: Glyph 3452 (\\N{MALAYALAM LETTER CHILLU RR}) missing from current font.\n",
      "  util.ensure_matplotlib_figure(data).savefig(buf, format=self.format)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.2399, Val Acc = 0.5090, Test Acc = 0.5171\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4a99a_row0_col0, #T_4a99a_row0_col1, #T_4a99a_row0_col2, #T_4a99a_row0_col3, #T_4a99a_row7_col0, #T_4a99a_row7_col1, #T_4a99a_row7_col2, #T_4a99a_row7_col3, #T_4a99a_row8_col0, #T_4a99a_row8_col1, #T_4a99a_row8_col2, #T_4a99a_row8_col3, #T_4a99a_row9_col0, #T_4a99a_row9_col1, #T_4a99a_row9_col2, #T_4a99a_row9_col3, #T_4a99a_row10_col0, #T_4a99a_row10_col1, #T_4a99a_row10_col2, #T_4a99a_row10_col3, #T_4a99a_row11_col0, #T_4a99a_row11_col1, #T_4a99a_row11_col2, #T_4a99a_row11_col3 {\n",
       "  background-color: lightpink;\n",
       "  text-align: left;\n",
       "}\n",
       "#T_4a99a_row1_col0, #T_4a99a_row1_col1, #T_4a99a_row1_col2, #T_4a99a_row1_col3, #T_4a99a_row2_col0, #T_4a99a_row2_col1, #T_4a99a_row2_col2, #T_4a99a_row2_col3, #T_4a99a_row3_col0, #T_4a99a_row3_col1, #T_4a99a_row3_col2, #T_4a99a_row3_col3, #T_4a99a_row4_col0, #T_4a99a_row4_col1, #T_4a99a_row4_col2, #T_4a99a_row4_col3, #T_4a99a_row5_col0, #T_4a99a_row5_col1, #T_4a99a_row5_col2, #T_4a99a_row5_col3, #T_4a99a_row6_col0, #T_4a99a_row6_col1, #T_4a99a_row6_col2, #T_4a99a_row6_col3, #T_4a99a_row12_col0, #T_4a99a_row12_col1, #T_4a99a_row12_col2, #T_4a99a_row12_col3, #T_4a99a_row13_col0, #T_4a99a_row13_col1, #T_4a99a_row13_col2, #T_4a99a_row13_col3, #T_4a99a_row14_col0, #T_4a99a_row14_col1, #T_4a99a_row14_col2, #T_4a99a_row14_col3 {\n",
       "  background-color: lightgreen;\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4a99a\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4a99a_level0_col0\" class=\"col_heading level0 col0\" >Input</th>\n",
       "      <th id=\"T_4a99a_level0_col1\" class=\"col_heading level0 col1\" >Predicted</th>\n",
       "      <th id=\"T_4a99a_level0_col2\" class=\"col_heading level0 col2\" >True</th>\n",
       "      <th id=\"T_4a99a_level0_col3\" class=\"col_heading level0 col3\" >Difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4a99a_row0_col0\" class=\"data row0 col0\" >amgathavavum</td>\n",
       "      <td id=\"T_4a99a_row0_col1\" class=\"data row0 col1\" >അംഗതവവും</td>\n",
       "      <td id=\"T_4a99a_row0_col2\" class=\"data row0 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_4a99a_row0_col3\" class=\"data row0 col3\" >അംഗത<b style=\"color:red\">വ</b>വ<b style=\"color:red\">ു</b><b style=\"color:red\">ം</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4a99a_row1_col0\" class=\"data row1 col0\" >amgathvavum</td>\n",
       "      <td id=\"T_4a99a_row1_col1\" class=\"data row1 col1\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_4a99a_row1_col2\" class=\"data row1 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_4a99a_row1_col3\" class=\"data row1 col3\" >അംഗത്വവും</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4a99a_row2_col0\" class=\"data row2 col0\" >angathwavum</td>\n",
       "      <td id=\"T_4a99a_row2_col1\" class=\"data row2 col1\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_4a99a_row2_col2\" class=\"data row2 col2\" >അംഗത്വവും</td>\n",
       "      <td id=\"T_4a99a_row2_col3\" class=\"data row2 col3\" >അംഗത്വവും</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4a99a_row3_col0\" class=\"data row3 col0\" >amgabalam</td>\n",
       "      <td id=\"T_4a99a_row3_col1\" class=\"data row3 col1\" >അംഗബലം</td>\n",
       "      <td id=\"T_4a99a_row3_col2\" class=\"data row3 col2\" >അംഗബലം</td>\n",
       "      <td id=\"T_4a99a_row3_col3\" class=\"data row3 col3\" >അംഗബലം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4a99a_row4_col0\" class=\"data row4 col0\" >angabalam</td>\n",
       "      <td id=\"T_4a99a_row4_col1\" class=\"data row4 col1\" >അംഗബലം</td>\n",
       "      <td id=\"T_4a99a_row4_col2\" class=\"data row4 col2\" >അംഗബലം</td>\n",
       "      <td id=\"T_4a99a_row4_col3\" class=\"data row4 col3\" >അംഗബലം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4a99a_row5_col0\" class=\"data row5 col0\" >amgeekarikkuka</td>\n",
       "      <td id=\"T_4a99a_row5_col1\" class=\"data row5 col1\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_4a99a_row5_col2\" class=\"data row5 col2\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_4a99a_row5_col3\" class=\"data row5 col3\" >അംഗീകരിക്കുക</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_4a99a_row6_col0\" class=\"data row6 col0\" >angeekarikkuka</td>\n",
       "      <td id=\"T_4a99a_row6_col1\" class=\"data row6 col1\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_4a99a_row6_col2\" class=\"data row6 col2\" >അംഗീകരിക്കുക</td>\n",
       "      <td id=\"T_4a99a_row6_col3\" class=\"data row6 col3\" >അംഗീകരിക്കുക</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_4a99a_row7_col0\" class=\"data row7 col0\" >ambaasadar</td>\n",
       "      <td id=\"T_4a99a_row7_col1\" class=\"data row7 col1\" >അംബാസർ</td>\n",
       "      <td id=\"T_4a99a_row7_col2\" class=\"data row7 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_4a99a_row7_col3\" class=\"data row7 col3\" >അംബാസ<b style=\"color:red\">ർ</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_4a99a_row8_col0\" class=\"data row8 col0\" >ambaassador</td>\n",
       "      <td id=\"T_4a99a_row8_col1\" class=\"data row8 col1\" >അംബാസർ</td>\n",
       "      <td id=\"T_4a99a_row8_col2\" class=\"data row8 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_4a99a_row8_col3\" class=\"data row8 col3\" >അംബാസ<b style=\"color:red\">ർ</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_4a99a_row9_col0\" class=\"data row9 col0\" >ambassador</td>\n",
       "      <td id=\"T_4a99a_row9_col1\" class=\"data row9 col1\" >അംബസ്ഡഡർ</td>\n",
       "      <td id=\"T_4a99a_row9_col2\" class=\"data row9 col2\" >അംബാസഡർ</td>\n",
       "      <td id=\"T_4a99a_row9_col3\" class=\"data row9 col3\" >അംബ<b style=\"color:red\">സ</b><b style=\"color:red\">്</b>ഡ<b style=\"color:red\">ഡ</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_4a99a_row10_col0\" class=\"data row10 col0\" >akathekku</td>\n",
       "      <td id=\"T_4a99a_row10_col1\" class=\"data row10 col1\" >അകത്തേക്ക്</td>\n",
       "      <td id=\"T_4a99a_row10_col2\" class=\"data row10 col2\" >അകത്തേക്കു</td>\n",
       "      <td id=\"T_4a99a_row10_col3\" class=\"data row10 col3\" >അകത്തേക്ക<b style=\"color:red\">്</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_4a99a_row11_col0\" class=\"data row11 col0\" >akkitham</td>\n",
       "      <td id=\"T_4a99a_row11_col1\" class=\"data row11 col1\" >അക്കിതം</td>\n",
       "      <td id=\"T_4a99a_row11_col2\" class=\"data row11 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_4a99a_row11_col3\" class=\"data row11 col3\" >അക്കിത<b style=\"color:red\">ം</b></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_4a99a_row12_col0\" class=\"data row12 col0\" >akkithham</td>\n",
       "      <td id=\"T_4a99a_row12_col1\" class=\"data row12 col1\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_4a99a_row12_col2\" class=\"data row12 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_4a99a_row12_col3\" class=\"data row12 col3\" >അക്കിത്തം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_4a99a_row13_col0\" class=\"data row13 col0\" >akkittham</td>\n",
       "      <td id=\"T_4a99a_row13_col1\" class=\"data row13 col1\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_4a99a_row13_col2\" class=\"data row13 col2\" >അക്കിത്തം</td>\n",
       "      <td id=\"T_4a99a_row13_col3\" class=\"data row13 col3\" >അക്കിത്തം</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4a99a_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_4a99a_row14_col0\" class=\"data row14 col0\" >accountil</td>\n",
       "      <td id=\"T_4a99a_row14_col1\" class=\"data row14 col1\" >അക്കൗണ്ടിൽ</td>\n",
       "      <td id=\"T_4a99a_row14_col2\" class=\"data row14 col2\" >അക്കൗണ്ടിൽ</td>\n",
       "      <td id=\"T_4a99a_row14_col3\" class=\"data row14 col3\" >അക്കൗണ്ടിൽ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to predictions_attention/test_predictions_embed256_hid128_enc2_dec1_LSTM_drop0.3_beam5_attn.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_35/2880310165.py\", line 398, in sweep_train\n",
      "    save_model(model, f'model_{run_name}.pt')\n",
      "  File \"/tmp/ipykernel_35/2880310165.py\", line 346, in save_model\n",
      "    'config': config\n",
      "              ^^^^^^\n",
      "NameError: name 'config' is not defined\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>test_accuracy</td><td>▁▅▆██</td></tr><tr><td>test_token_accuracy</td><td>▁▆▇██</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▅▇██</td></tr><tr><td>val_token_accuracy</td><td>▁▆▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>5</td></tr><tr><td>test_accuracy</td><td>0.51711</td></tr><tr><td>test_token_accuracy</td><td>0.83063</td></tr><tr><td>train_loss</td><td>0.2399</td></tr><tr><td>used_attention</td><td>True</td></tr><tr><td>val_accuracy</td><td>0.50895</td></tr><tr><td>val_token_accuracy</td><td>0.82101</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">embed256_hid128_enc2_dec1_LSTM_drop0.3_beam5_attn</strong> at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/xhz3l7hg' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020/runs/xhz3l7hg</a><br> View project at: <a href='https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020' target=\"_blank\">https://wandb.ai/apoorvaprashanth-indian-institute-of-technology-madras/A3_ce21b020</a><br>Synced 5 W&B file(s), 3 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250520_183307-xhz3l7hg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xhz3l7hg errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_35/2880310165.py\", line 398, in sweep_train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     save_model(model, f'model_{run_name}.pt')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_35/2880310165.py\", line 346, in save_model\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     'config': config\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m NameError: name 'config' is not defined\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip created: predictions_attention.zip\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Char-level vocabulary with encoding specification\n",
    "class CharVocab:\n",
    "    def __init__(self, words):\n",
    "        chars = sorted(set(\"\".join(words)))\n",
    "        self.char2idx = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n",
    "        for c in chars:\n",
    "            self.char2idx[c] = len(self.char2idx)\n",
    "        self.idx2char = {i: c for c, i in self.char2idx.items()}\n",
    "        self.pad_idx = self.char2idx['<pad>']\n",
    "        self.sos_idx = self.char2idx['<sos>']\n",
    "        self.eos_idx = self.char2idx['<eos>']\n",
    "\n",
    "    def encode(self, word):\n",
    "        return [self.sos_idx] + [self.char2idx.get(c, self.char2idx['<unk>']) for c in word] + [self.eos_idx]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        chars = []\n",
    "        for idx in ids:\n",
    "            if idx == self.eos_idx:\n",
    "                break\n",
    "            if idx not in (self.sos_idx, self.pad_idx):\n",
    "                chars.append(self.idx2char.get(idx, ''))\n",
    "        return ''.join(chars)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.char2idx)\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        lines = f.read().strip().split('\\n')\n",
    "    return [(line.split('\\t')[0], line.split('\\t')[1]) for line in lines if len(line.split('\\t')) >= 2]\n",
    "\n",
    "train_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.train.tsv')\n",
    "dev_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.dev.tsv')\n",
    "test_pairs = read_file('/kaggle/input/malayalam/ml.translit.sampled.test.tsv')\n",
    "\n",
    "src_vocab = CharVocab([src for _, src in train_pairs])\n",
    "tgt_vocab = CharVocab([tgt for tgt, _ in train_pairs])\n",
    "\n",
    "class TransliterationDataset(Dataset):\n",
    "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
    "        self.data = pairs\n",
    "        self.src_vocab = src_vocab\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tgt, src = self.data[idx]\n",
    "        return torch.tensor(self.src_vocab.encode(src)), torch.tensor(self.tgt_vocab.encode(tgt))\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "    src_pad = nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=src_vocab.pad_idx)\n",
    "    tgt_pad = nn.utils.rnn.pad_sequence(tgt_batch, batch_first=True, padding_value=tgt_vocab.pad_idx)\n",
    "    return src_pad, tgt_pad\n",
    "\n",
    "train_loader = DataLoader(TransliterationDataset(train_pairs, src_vocab, tgt_vocab),\n",
    "                          batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader = DataLoader(TransliterationDataset(dev_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(TransliterationDataset(test_pairs, src_vocab, tgt_vocab),\n",
    "                        batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hidden_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.v = nn.Parameter(torch.rand(dec_hidden_dim))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        energy = energy.permute(0, 2, 1)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        attention = torch.bmm(v, energy).squeeze(1)\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, config, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = config.embed_size\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_enc_layers = config.enc_layers\n",
    "        self.num_dec_layers = config.dec_layers\n",
    "        self.cell_type = config.cell\n",
    "        self.device = device\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.max_len = 30\n",
    "        self.attention_weights = []  # Store attention weights for visualization\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.decoder_embedding = nn.Embedding(output_vocab_size, self.embedding_dim)\n",
    "\n",
    "        RNN = {'RNN': nn.RNN, 'GRU': nn.GRU, 'LSTM': nn.LSTM}[self.cell_type]\n",
    "        self.encoder = RNN(self.embedding_dim, self.hidden_size, num_layers=self.num_enc_layers,\n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.decoder = RNN(self.embedding_dim + self.hidden_size * 2, self.hidden_size * 2,\n",
    "                           num_layers=self.num_dec_layers, batch_first=True)\n",
    "\n",
    "        self.attention = Attention(self.hidden_size * 2, self.hidden_size * 2)\n",
    "        self.fc = nn.Linear(self.hidden_size * 4, output_vocab_size)\n",
    "\n",
    "        self.sos_idx = tgt_vocab.sos_idx\n",
    "        self.eos_idx = tgt_vocab.eos_idx\n",
    "        self.pad_idx = tgt_vocab.pad_idx\n",
    "\n",
    "    def encode(self, src):\n",
    "        embedded = self.dropout(self.encoder_embedding(src))\n",
    "        outputs, h_n = self.encoder(embedded)\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h, c = h_n\n",
    "            h_cat = torch.cat((h[-2], h[-1]), dim=1).unsqueeze(0)\n",
    "            c_cat = torch.cat((c[-2], c[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, (h_cat, c_cat)\n",
    "        else:\n",
    "            h_cat = torch.cat((h_n[-2], h_n[-1]), dim=1).unsqueeze(0)\n",
    "            return outputs, h_cat\n",
    "\n",
    "    def decode_step(self, input_token, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.decoder_embedding(input_token))\n",
    "        if self.cell_type == 'LSTM':\n",
    "            h_t = hidden[0][-1]\n",
    "        else:\n",
    "            h_t = hidden[-1]\n",
    "            \n",
    "        attn_weights = self.attention(h_t, encoder_outputs)\n",
    "        self.attention_weights.append(attn_weights)  # Store for visualization\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.decoder(rnn_input, hidden)\n",
    "        logits = self.fc(torch.cat((output.squeeze(1), context.squeeze(1)), dim=1))\n",
    "        return logits, hidden, attn_weights\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        self.attention_weights = []  # Reset attention weights storage\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        encoder_outputs, hidden = self.encode(src)\n",
    "        input_token = tgt[:, 0].unsqueeze(1)\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, attn_weights = self.decode_step(input_token, hidden, encoder_outputs)\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1).unsqueeze(1)\n",
    "            input_token = tgt[:, t].unsqueeze(1) if teacher_force else top1\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        loss = criterion(output.view(-1, output.size(-1)), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def beam_decode(model, src, beam_size):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden = model.encode(src)\n",
    "        batch_size = src.size(0)\n",
    "        final_outputs = []\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            h_b = (hidden[0][:, b:b+1, :].contiguous(), hidden[1][:, b:b+1, :].contiguous()) if model.cell_type == 'LSTM' else hidden[:, b:b+1, :].contiguous()\n",
    "            enc_out_b = encoder_outputs[b:b+1]\n",
    "            beams = [([model.sos_idx], 0.0, h_b)]\n",
    "            \n",
    "            for _ in range(model.max_len):\n",
    "                new_beams = []\n",
    "                for seq, score, h in beams:\n",
    "                    if seq[-1] == model.eos_idx:\n",
    "                        new_beams.append((seq, score, h))\n",
    "                        continue\n",
    "                    input_token = torch.tensor([[seq[-1]]], device=device)\n",
    "                    out, h_new, _ = model.decode_step(input_token, h, enc_out_b)\n",
    "                    log_probs = F.log_softmax(out, dim=1)\n",
    "                    topk_probs, topk_idxs = torch.topk(log_probs, beam_size, dim=1)\n",
    "                    for i in range(beam_size):\n",
    "                        next_seq = seq + [topk_idxs[0][i].item()]\n",
    "                        new_score = score + topk_probs[0][i].item()\n",
    "                        new_beams.append((next_seq, new_score, h_new))\n",
    "                beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            final_outputs.append(beams[0][0])\n",
    "        return final_outputs\n",
    "\n",
    "def plot_attention_heatmaps(model, test_samples, num_samples=9):\n",
    "    model.eval()\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    samples = test_samples[:num_samples]\n",
    "    \n",
    "    for i, (src, tgt) in enumerate(samples):\n",
    "        plt.subplot(3, 3, i+1)\n",
    "        with torch.no_grad():\n",
    "            src_tensor = torch.tensor([src_vocab.encode(src)], device=device)\n",
    "            tgt_tensor = torch.tensor([tgt_vocab.encode(tgt)], device=device)\n",
    "            model(src_tensor, tgt_tensor)  # This populates attention_weights\n",
    "            \n",
    "            # Get attention weights and convert to numpy\n",
    "            attn_weights = torch.cat(model.attention_weights).squeeze().cpu().numpy()\n",
    "            \n",
    "            # Create heatmap with proper labels\n",
    "            ax = sns.heatmap(attn_weights, cmap=\"YlGnBu\", \n",
    "                        xticklabels=list(src),\n",
    "                        yticklabels=list(tgt))\n",
    "            plt.title(f\"Input: {src}\\nOutput: {tgt}\")\n",
    "            plt.xlabel(\"Source Characters\")\n",
    "            plt.ylabel(\"Target Characters\")\n",
    "            ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "            ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt\n",
    "\n",
    "def create_interactive_attention_plot(model, src_word, tgt_word):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        src_tensor = torch.tensor([src_vocab.encode(src_word)], device=device)\n",
    "        tgt_tensor = torch.tensor([tgt_vocab.encode(tgt_word)], device=device)\n",
    "        model(src_tensor, tgt_tensor)\n",
    "        \n",
    "        attn_weights = torch.cat(model.attention_weights).squeeze().cpu().numpy()\n",
    "        \n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=attn_weights,\n",
    "            x=list(src_word),\n",
    "            y=list(tgt_word),\n",
    "            colorscale='YlGnBu',\n",
    "            hoverongaps=False\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f'Attention Visualization: {src_word} → {tgt_word}',\n",
    "            xaxis_title='Source Characters',\n",
    "            yaxis_title='Target Characters',\n",
    "            width=800,\n",
    "            height=600\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "def evaluate_beam(model, dataloader, beam_size):\n",
    "    model.eval()\n",
    "    total_seq, correct_seq = 0, 0\n",
    "    total_tokens, correct_tokens = 0, 0\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            preds = beam_decode(model, src, beam_size)\n",
    "            \n",
    "            for i, (pred, true) in enumerate(zip(preds, tgt)):\n",
    "                pred_trimmed = [tok for tok in pred[1:] if tok != model.pad_idx and tok != model.eos_idx]\n",
    "                true_trimmed = [tok.item() for tok in true[1:] if tok.item() != model.pad_idx and tok.item() != model.eos_idx]\n",
    "\n",
    "                if pred_trimmed == true_trimmed:\n",
    "                    correct_seq += 1\n",
    "                total_seq += 1\n",
    "\n",
    "                for p, t in zip(pred_trimmed, true_trimmed):\n",
    "                    if p == t:\n",
    "                        correct_tokens += 1\n",
    "                total_tokens += len(true_trimmed)\n",
    "                \n",
    "                src_word = src_vocab.decode([x.item() for x in src[i] if x.item() not in (src_vocab.sos_idx, src_vocab.eos_idx, src_vocab.pad_idx)])\n",
    "                pred_word = tgt_vocab.decode(pred)\n",
    "                true_word = tgt_vocab.decode([x.item() for x in true if x.item() not in (tgt_vocab.pad_idx, tgt_vocab.eos_idx)])\n",
    "                \n",
    "                all_predictions.append((src_word, pred_word, true_word))\n",
    "\n",
    "    seq_accuracy = correct_seq / total_seq if total_seq > 0 else 0.0\n",
    "    token_accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0.0\n",
    "    return seq_accuracy, token_accuracy, all_predictions\n",
    "\n",
    "def visualize_predictions(predictions, num_samples=10, log_to_wandb=False):\n",
    "    df = pd.DataFrame(predictions[:num_samples], columns=['Input', 'Predicted', 'True'])\n",
    "    \n",
    "    def highlight_diff(row):\n",
    "        pred, true = row['Predicted'], row['True']\n",
    "        diff = []\n",
    "        for p, t in zip(pred, true):\n",
    "            if p == t:\n",
    "                diff.append(p)\n",
    "            else:\n",
    "                diff.append(f'<b style=\"color:red\">{p}</b>')\n",
    "        return ''.join(diff)\n",
    "    \n",
    "    df['Difference'] = df.apply(lambda row: highlight_diff(row), axis=1)\n",
    "    \n",
    "    def row_style(row):\n",
    "        color = 'lightgreen' if row['Predicted'] == row['True'] else 'lightpink'\n",
    "        return [f'background-color: {color}' for _ in row]\n",
    "    \n",
    "    styled_df = df.style.apply(row_style, axis=1).set_properties(**{'text-align': 'left'})\n",
    "    display(HTML(styled_df.to_html(escape=False)))\n",
    "    \n",
    "    if log_to_wandb:\n",
    "        wandb.log({\"predictions\": wandb.Table(dataframe=df)})\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "def save_predictions(predictions, filename):\n",
    "    os.makedirs('predictions_attention', exist_ok=True)\n",
    "    df = pd.DataFrame(predictions, columns=['Input', 'Predicted', 'True'])\n",
    "    # Save with UTF-8 encoding to handle special characters\n",
    "    df.to_csv(f'predictions_attention/{filename}', index=False, encoding='utf-8')\n",
    "    print(f\"Saved to predictions_attention/{filename}\")\n",
    "\n",
    "def save_model(model, filename):\n",
    "    os.makedirs('saved_models', exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'src_vocab': src_vocab,\n",
    "        'tgt_vocab': tgt_vocab,\n",
    "        'config': config\n",
    "    }, f'saved_models/{filename}')\n",
    "    print(f\"Model saved to saved_models/{filename}\")\n",
    "\n",
    "\n",
    "def sweep_train(config=None):\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "        run_name = f\"embed{config.embed_size}_hid{config.hidden_size}_enc{config.enc_layers}_dec{config.dec_layers}_{config.cell}_drop{config.dropout}_beam{config.beam_size}_attn\"\n",
    "        wandb.run.name = run_name\n",
    "\n",
    "        model = Seq2Seq(config, len(src_vocab), len(tgt_vocab)).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=tgt_vocab.pad_idx)\n",
    "\n",
    "        for epoch in range(5):\n",
    "            train_loss = train(model, train_loader, optimizer, criterion)\n",
    "            acc, token_acc, val_preds = evaluate_beam(model, dev_loader, beam_size=config.beam_size)\n",
    "            test_acc, test_token_acc, test_preds = evaluate_beam(model, test_loader, beam_size=config.beam_size)\n",
    "\n",
    "            # Log attention visualizations\n",
    "            if epoch == 4:\n",
    "                # Static heatmaps\n",
    "                test_samples = []\n",
    "                for i in range(9):\n",
    "                    src, tgt = test_loader.dataset[i]\n",
    "                    src_word = src_vocab.decode([x.item() for x in src if x.item() not in (src_vocab.sos_idx, src_vocab.eos_idx, src_vocab.pad_idx)])\n",
    "                    tgt_word = tgt_vocab.decode([x.item() for x in tgt if x.item() not in (tgt_vocab.pad_idx, tgt_vocab.eos_idx)])\n",
    "                    test_samples.append((src_word, tgt_word))\n",
    "                \n",
    "                attention_plot = plot_attention_heatmaps(model, test_samples)\n",
    "                wandb.log({\"attention_heatmaps\": wandb.Image(attention_plot)})\n",
    "                plt.close()\n",
    "                \n",
    "                # Interactive visualization for one example\n",
    "                src_word, tgt_word = test_samples[0]\n",
    "                interactive_fig = create_interactive_attention_plot(model, src_word, tgt_word)\n",
    "                wandb.log({\"interactive_attention\": wandb.Plotly(interactive_fig)})\n",
    "\n",
    "            wandb.log({\n",
    "                'epoch': epoch + 1,\n",
    "                'train_loss': train_loss,\n",
    "                'val_accuracy': acc,\n",
    "                'val_token_accuracy': token_acc,\n",
    "                'test_accuracy': test_acc,\n",
    "                'test_token_accuracy': test_token_acc,\n",
    "                'used_attention': True\n",
    "            })\n",
    "\n",
    "            print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Acc = {acc:.4f}, Test Acc = {test_acc:.4f}\")\n",
    "        \n",
    "        visualize_predictions(test_preds, num_samples=15, log_to_wandb=True)\n",
    "        save_predictions(test_preds, f'test_predictions_{run_name}.csv')\n",
    "        save_model(model, f'model_{run_name}.pt')\n",
    "\n",
    "# Sweep Config\n",
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'metric': {'name': 'val_accuracy', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'embed_size': {'values': [256]},\n",
    "        'hidden_size': {'values': [128]},\n",
    "        'enc_layers': {'values': [2]},\n",
    "        'dec_layers': {'values': [1]},\n",
    "        'dropout': {'values': [0.30]},\n",
    "        'cell': {'values': ['LSTM']},\n",
    "        'beam_size': {'values': [5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"A3_ce21b020\")\n",
    "wandb.agent(sweep_id, function=sweep_train, count=1)\n",
    "\n",
    "def create_prediction_zip():\n",
    "    with zipfile.ZipFile('predictions_attention.zip', 'w') as zipf:\n",
    "        for root, dirs, files in os.walk('predictions_attention'):\n",
    "            for file in files:\n",
    "                zipf.write(os.path.join(root, file))\n",
    "    print(\"Zip created: predictions_attention.zip\")\n",
    "\n",
    "create_prediction_zip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error above means that the font being used by Matplotlib (and WandB) does not support some Malayalam characters, so those characters cannot be rendered (displayed correctly) in plots or images. Beacuse of which, some chanracters will be shown as blocks. Howvwer, this doesn't affect the functionality of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7443161,
     "sourceId": 11846275,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
